#+TITLE: Parsing Gigabytes of JSON per Second
#+AUTHOR: Wong Ding Feng
#+LANGUAGE: en
#+OPTIONS: num_lines:t toc:1 ^:nil
#+REVEAL_THEME: moon
#+EXPORT_SELECT_STRINGS: ((org-export-string "latex") "\\usepackage{amsmath} \\usepackage{amsthm} \\usepackage{amssymb}")
#+REVEAL_HEAD_PREAMBLE: <style> .reveal { font-size: 32px; } .reveal .slides { margin: 0.5em; } </style>
* Objectives
#+begin_notes
Too many things to talk about
pick and choose some
#+end_notes
- why is JSON slow?
- Simple primer on bitwise operations and simd
- simdjson architecture
- C++ techniques used
* Problem
** SBE vs JSON
#+begin_notes
the reason why sbe is because we have the schema
- we know how far to jump in the array to get data
- there is no key to read and guess
- sometimes we have the offset to jump forward

while for json we need to guess what the data means
there is also validation
#+end_notes
#+begin_src text
Binary Format (Schema: string[10], uint8)
┌─────────────────┬───────┐
│ "John Doe"      │ 42    │
└─────────────────┴───────┘
  |               └─ Age: Fixed 1 byte, parser knows to read exactly 1 byte
  └─ Name: Fixed 10 bytes, parser knows to read exactly 10 bytes
                     (padded with spaces)

JSON Format
┌──────────────────────────────────┐
│ {"name":"John Doe","age":42}     │
└──────────────────────────────────┘
  |     |           |     └─ Parser must scan until it finds closing brace
  |     |           └─ Parser must scan for quotes and ":"
  |     └─ Parser must scan for quotes and ":"
  └─ Parser must scan character by character, looking for valid JSON tokens
#+end_src
** Why is json interesting?
#+begin_notes
- basically double the speed means you can parse 2x as much data
- duplicating hardware for free
#+end_notes
- most data is in json
| parser    | Skylake | Cannon Lake | speed  |
|-----------+---------+-------------+--------|
| simdjson  |     1.4 |         1.3 | fast   |
| RapidJSON |    0.56 |        0.44 | slow   |
| sajson    |    0.93 |        0.84 | normal |
** Why you should be interested
#+begin_notes
this is the breakdown
can configure, if you remove utf-8 check
its even faster
#+end_notes
- configurable, increase speed
#+ATTR_HTML: :style background-color: white;
[[https://arxiv.org/html/1902.08318v7/x1.png]]
** Against others
#+ATTR_HTML: :style background-color: white;
[[https://arxiv.org/html/1902.08318v7/x3.png]]
** On demand json
#+ATTR_HTML: :style background-color: white;
[[https://arxiv.org/html/2312.17149v3/x1.png]]
** Usage
- read
  - simdjson
  - simdjson: On Demand (stream)
  - fastjson2
- write
  - rapidjson
  - fastjson2
* Ideas on how to do it fast?
#+begin_notes
Look at how other people tried to speed it up
so what other ways can we speed it up?
json
#+end_notes
** Strategies
- depends on the usage pattern
*** Query intensive
- Create a database (ElasticSearch, MongoDB, PostgresSQL)
  - create a KV store
  - load once and query it
*** Selective parsing
#+begin_notes
The other opposite end is no parsing,
Skip the parsing as parsing takes time, NoDB, do a grep search and jump around detecting some structures and patterns in the data
There was a research paper talking about using JIT and speeding up the json query like a compiler
Mison is another implementation that uses simd to find important character locations like braces [] " : and the authors of simdjson learnt lots from them.
#+end_notes
- Selective parsing
  - NoDB
    - query the data without parsing it, without loading into a DB
    - like grep
  - JIT techniques
    - find patterns and repetitive structures, compile the code for the specific query
    - like a compiler
  - *Mison* (by Microsoft)
    - selective parsing, jump directly to the field you want
    - use SIMD to find structural important characters like "
** What is fair game?
#+begin_notes
So there are many json implementations out there and to measure performance, we need to properly define what json means.

Most faster json parser implementations play cheat by just assuming the input is already valid.

assuming strings are only ascii when json RFC said UTF-8, dont validate numbers, selectively parsing.

simdjson is a complete parser following JSON RFC standards, fully validating the input yet being faster than all of them. assuming input is correct is dangerous because it just is wrong input and wrong output
#+end_notes
- Types of json parsing
  - Non-validating json parser
    - assume the input is valid
    - easier
    - most selective parsing is non-validating
  - Validating json parser
    - check the input is valid
    - no assumptions or malformed input
      - security risk
      - its just wrong number or string being parsed
    - harder more complex
** Proper definition of JSON
#+begin_notes
This is the real EBNF grammar for json, its kinda complex so i wrote a simplified version below
#+end_notes

#+begin_src ebnf
/* JSON EBNF Grammar Specification */

/* Root JSON structure */
json = ws , (object | array) , ws ;

/* Objects */
object = "{" , ws , [ members ] , ws , "}" ;
members = pair , { "," , ws , pair } ;
pair = string , ws , ":" , ws , value ;

/* Arrays */
array = "[" , ws , [ elements ] , ws , "]" ;
elements = value , { "," , ws , value } ;

/* Values */
value = string | number | object | array | "true" | "false" | "null" ;

/* Strings */
string = '"' , { char | escape } , '"' ;
char = ? any Unicode character except " or \ or control characters ? ;
escape = "\" , ('"' | "\" | "/" | "b" | "f" | "n" | "r" | "t" | unicode) ;
unicode = "u" , hexdigit , hexdigit , hexdigit , hexdigit ;
hexdigit = digit | "A" | "B" | "C" | "D" | "E" | "F" | "a" | "b" | "c" | "d" | "e" | "f" ;

/* Numbers */
number = [ "-" ] , (zero | integer) , [ fraction ] , [ exponent ] ;
integer = nonzero , { digit } ;
nonzero = "1" | "2" | "3" | "4" | "5" | "6" | "7" | "8" | "9" ;
digit = "0" | nonzero ;
zero = "0" ;
fraction = "." , digit , { digit } ;
exponent = ("E" | "e") , [ "+" | "-" ] , digit , { digit } ;

/* Whitespace */
ws = { whitespace } ;
whitespace = " " | "\t" | "\n" | "\r" ;

/* Comments and Explanation */
#+end_src
** Strongly define: bool, string, number, null, object and array
#+begin_notes
very simple, we just need to strongly define these 6 basic types
bool, string, number, null, object and array
write parsing rules to validate and check that they are valid
then understand what the data means

This sounds simple, but it is deceptively simple
#+end_notes
#+begin_src haskell
data JsonValue
    = Primitive PrimitiveValue
    | Container ContainerValue

-- 6 primitives --------------------------
data PrimitiveValue
    = Boolean Bool  -- true | false
    | String Text   -- "string"
    | Number Double -- 123, 1.23, 123e0, 123E0
    | Null          -- null

data ContainerValue
    = Object Object -- { "string", PrimitiveValue, ... }
    | Array Array   -- [ PrimitiveValue, ... ]
-- END -----------------------------------

newtype Object = Object [(Text, JsonValue)]
newtype Array = Array [JsonValue]
#+end_src
** Strongly define: bool, string, number, null, object and array
#+begin_notes
boolean, true, false and null are trivial

So first we start with the simplest sounding one of all, number, just integers right?, decimal perhaps? easy!
#+end_notes
*** Number limits and Integers
#+begin_notes
Lets take a look at limits.
Theres no strict definition for the limit of numbers, most use (2^53 - 1) because of the floating point representation
the authors of SIMDjson prefer 2^63 - 1
the first special case we have to deal with is negative numbers, we cant only detect 0 - 9, we have to detect - as well.
#+end_notes
#+begin_src javascript
// 1. Integer Limits
const INTEGER_EXAMPLES = {
    // Maximum safe integer in JavaScript (2^53 - 1)
    max_safe_integer: 9007199254740991,
    // Minimum safe integer in JavaScript (-(2^53 - 1))
    min_safe_integer: -9007199254740991,

    // Zero representations
    zero: 0,
    negative_zero: -0,  // JSON preserves negative zero

    // Common boundary values
    max_32bit_int: 2147483647,
    min_32bit_int: -2147483648,

    // Integer examples
    positive: 42,
    negative: -42
};
#+end_src
*** Floats and Scientific notation
#+begin_notes
Floats, you see the e-308.
below you can see that both E and e are valid
some + and some - exponents
some without the + and - signs

what about the special case of 0.0e0!? how do we handle that?
these are all the details your validator needs to check for before declaring that this is a valid input
#+end_notes
#+begin_src javascript
// 2. Floating Point Examples
const FLOAT_EXAMPLES = {
    // Precision examples (up to 15-17 significant digits)
    high_precision: 1.234567890123456,

    // Edge cases
    very_small_positive: 2.2250738585072014e-308, // Near smallest possible double
    very_large_positive: 1.7976931348623157e+308  // Near largest possible double
};

// 3. Scientific Notation Examples
const SCIENTIFIC_NOTATION = {
    // Positive exponents
    large_scientific: 1.23e+11,
    very_large: 1.23E+308,  // Note: Both 'e' and 'E' are valid

    // Negative exponents
    small_scientific: 1.23e-11,
    very_small: 1.23E-308,

    // Zero with exponent
    zero_scientific: 0.0e0,

    // Various representations
    alternative_forms: {
        standard: 1230000000,
        scientific: 1.23e9,
        another_form: 123e7
    }
};
#+end_src
** String: handle escaped quotes and UTF-8
#+begin_notes
next we have string, many implementations just assume ascii
- simple 128 possibilities
- mostly correct data usually just ascii

but json RFC says it must be UTF-8
the last important thing to take note of is escaped \", we need to detect them properly to get the correct json, everything is done in simd.
#+end_notes
- some lazy parsers assume ascii for simplicity
  - 128 possibilities, 8 bits only
  - assume that input does not have japanese or chinese or weird characters
- RFC standard says strings are UTF-8
- escaped double quotes "Tom said: \"hello\"."
  - Tom said: "hello".
  - number of '\'
    - odd -> escaped, "\"" -> "
    - even -> not escaped, "\\" -> \
- outside of ",there can only be 4 types of white space
  - " " | "\t" | "\r" | "\n"
*** ASCII code
#+begin_notes
This is just simply the ascii code table, quite sure we are all very familiar with it 0x30 - 0x39 is digits 0-9 lets move on
#+end_notes
- code ponits 0x00 - 0xEF 127 possibilities
#+ATTR_HTML: :style background-color: white;
[[https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/USASCII_code_chart.svg/1280px-USASCII_code_chart.svg.png]]
*** UTF-8
#+begin_notes
if it starts with the first bit being 0, it is ASCII
else if it is 1, it must conform to UTF-8 standards

this is why utf-8 validation is not straight forward
but the authors fugired out a way to do it with simd
it was able to detect this pattern in simd
#+end_notes
#+begin_src text
Single byte (ASCII):
0xxxxxxx                     (values 0-127)
Values start with 0, remaining 7 bits for data

Two bytes:
110xxxxx 10xxxxxx           (values 128-2047)
First byte starts with 110

Three bytes:
1110xxxx 10xxxxxx 10xxxxxx  (values 2048-65535)
First byte starts with 1110

Four bytes:
11110xxx 10xxxxxx 10xxxxxx 10xxxxxx   (values 65536+)
First byte starts with 11110
#+end_src
** Summary of requirements
- numbers
  - negative + -
  - floats 1.23
- string
  - utf-8
  - escaped quotes \" | \\"
- Rest of structure well formed
  - valid whitespace
  - valid bracket {}, []
* Challenges
** Writing a parser for it
#+begin_notes
Im not sure how many of us here has written a recursive descent parser but normally
how one would write a parser is that
one would just scan the string from left to right until it can determine what to do with the input
this requires many if else checks to see when to stop, when to look back, when to decide that what I am seeing is an object, string, array ...

the problem is that if statements cause a miss predicted branch, this is very costly to computers
if the branching is very predictable, like taking true all the time, there is no cost, usually the cpu will just
assume the previous branch was taken and follow that, then the cost is 1 cycle

if we need to stop and recorrect the branch it will take at least 15 cycles.

Can you do it without branches? thats what the SIMDjson team was working on.
#+end_notes
- Recursive Descent type parser
- Many if else required, is it possible to do it without any branches?
#+begin_src python
def peek_token_type(json_str, index):
    char = json_str[index]

    # Skip whitespace
    while index < len(json_str) and is_whitespace(char):
        index += 1
        char = json_str[index]

    # Check data type based on first character
    if char == '{':
        return 'object'
    elif char == '[':
        return 'array'
    elif char == '"':
        return 'string'
    elif is_digit(char):
        return 'number'
    elif char == 't' or char == 'f':
        return 'boolean'
    elif char == 'n':
        return 'null'
    else:
        raise ValueError(f"Invalid JSON character at position {index}: {char}")
#+end_src
** Given the challenge, how to do it fast?
#+begin_notes
mison already implemented some of these but not everything
#+end_notes
- SIMD, process more than 8 bytes at a time.
  - Branchless code, no if statements. CPU missed branch prediction.
    - correct, 0-1 cycles
    - branch miss, 20 cycles
* About SIMD
how does simd fit into all of this?
** What is simd
#+begin_notes
usually when we program, we work with one variable 64 bytes
this is the sisd model
one data one instruction

just imagine that instead of working with one number
we work with 8 in parallel instead 256, 512 bytes
this is the simd on the bottom left
#+end_notes
[[https://pep-root6.github.io/docs/analysis/simd.png]]
** SIMD example
#+begin_notes
here is an example
its just adding 2 arrays together with one instruction

normally we need a for loop to do this

but using simd we can do it in one step
#+end_notes
#+BEGIN_EXAMPLE
Adding 4 numbers simultaneously:

Scalar:
A: [5] + [3] = [8]     Step 1
B: [7] + [2] = [9]     Step 2
C: [4] + [6] = [10]    Step 3
D: [1] + [8] = [9]     Step 4

SIMD:
[5|7|4|1] +
[3|2|6|8] =   Step 1
[8|9|10|9]    Done!
#+END_EXAMPLE
** CPU
#+begin_src text
Year:         2010          2013          2019
Architecture: Westmere  ->  Haswell   ->  Ice Lake
Process:      32nm          22nm          10nm
Vector ISA:   SSE2      ->  AVX2      ->  AVX512
Vec Width:    128-bit       256-bit       512-bit
             (16 bytes)    (32 bytes)    (64 bytes)
#+end_src
- Streaming SIMD Extensions
  - XMM0-XMM15
- Advanced Vector Extensions 2
  - YMM0-YMM15
- Advanced Vector Extensions 512
  - ZMM0-ZMM15
** SIMD code is not that scary
Westmere uses 128-bit SSE instructions     (_mm_shuffle_epi8)
Haswell  uses 256-bit AVX2 instructions    (_mm256_shuffle_epi8)
Ice Lake uses 512-bit AVX-512 instructions (_mm512_shuffle_epi8)
#+begin_src cpp
// Westmere
const uint64_t whitespace = in.eq({
    _mm_shuffle_epi8(whitespace_table, in.chunks[0]),
    _mm_shuffle_epi8(whitespace_table, in.chunks[1]),
    _mm_shuffle_epi8(whitespace_table, in.chunks[2]),
    _mm_shuffle_epi8(whitespace_table, in.chunks[3])
});

// Haswell (2 x 256-bit chunks)
const uint64_t whitespace = in.eq({
    _mm256_shuffle_epi8(whitespace_table, in.chunks[0]),
    _mm256_shuffle_epi8(whitespace_table, in.chunks[1])
});

// Ice Lake (1 x 512-bit chunk)
const uint64_t whitespace = in.eq({
    _mm512_shuffle_epi8(whitespace_table, in.chunks[0])
});
#+end_src
** Some simd example
#+begin_notes
add
have different types like 8,16,32,64
subtract
multiply
or and xor bitwise
sll srl
#+end_notes
#+ATTR_HTML: :style font-size: 16px;
| Intrinsic Function         | Instruction | Description                 |
|----------------------------+-------------+-----------------------------|
| _mm256_add_epi8(a, b)      | VPADDB      | Add packed 8-bit            |
| _mm256_add_epi16(a, b)     | VPADDW      | Add packed 16-bit           |
| _mm256_add_epi32(a, b)     | VPADDD      | Add packed 32-bit           |
| _mm256_add_epi64(a, b)     | VPADDQ      | Add packed 64-bit           |
| _mm256_sub_epi64(a, b)     | VPSUBQ      | Subtract packed 64-bit      |
| _mm256_mullo_epi32(a, b)   | VPMULLD     | Multiply packed 32-bit      |
| _mm256_mulhi_epi16(a, b)   | VPMULHW     | Multiply packed 16-bit      |
| _mm256_and_si256(a, b)     | VPAND       | Bitwise AND of 256 bits     |
| _mm256_or_si256(a, b)      | VPOR        | Bitwise OR of 256 bits      |
| _mm256_xor_si256(a, b)     | VPXOR       | Bitwise XOR of 256 bits     |
| _mm256_andnot_si256(a, b)  | VPANDN      | Bitwise AND NOT of 256 bits |
| _mm256_slli_epi64(a, imm8) | VPSLLQ      | Shift packed 64-bit         |
| _mm256_srli_epi64(a, imm8) | VPSRLQ      | Shift packed 64-bit         |
** When SIMD Shines
#+begin_notes
- Regular, predictable data patterns
- Simple mathematical operations
- Continuous blocks of memory
- Identical operations on multiple data points
- High throughput
#+end_notes
- Regular, predictable data patterns
- Simple mathematical operations
- Continuous blocks of memory
- Identical operations on multiple data points
- High throughput
#+begin_src text
Perfect for SIMD:
[1|2|3|4] × 2  = [2 |4 |6 |8 ] ✓
[R|G|B|A] + 10 = [R'|G'|B'|A'] ✓
#+end_src
** SIMD's Achilles Heel: Branching
#+begin_notes
is harder to do simd

but if we can remove the if statements
it is possible to do simd

we do it with branchless, simd can be applied to it
#+end_notes
- if logic is complex like in parsing unable to do simd
#+begin_src c++
    if (char_at == '{') {
        return "object";
    } else if (char_at == '[') {
        return "array";
    } else if (char_at == '"') {
        return "string";
    } else if (is_digit(char_at)) {
        return "number";
    } else if (char_at == 't' || char_at == 'f') {
        return "boolean";
    } else if (char_at == 'n') {
        return "null";
    } else {
        throw std::invalid_argument(
            "Invalid JSON character at position " +
            std::to_string(index) +
            ": " + char_at
        );
    }
#+end_src
*** Correct branch prediction
#+begin_notes
when there is a branch, branch prediction usually use the last 2 times rule
and assumes that that branch will be taken

this is the cpu instruction pipeline
there are 5 steps in the cpu
fetch, decode, execue, memory, write
so they pipeline so that every time, all stages are busy

if they guess the branch correctly, there is actually no cost
#+end_notes
#+begin_src text
IF = Instruction Fetch
ID = Instruction Decode
EX = Execute
MEM = Memory Access
WB = Write Back

Time →
1  2  3  4  5  6  7  8  9
IF ID EX ME WB          | Instruction 1 (branch)
   IF ID EX ME WB       | Instruction 2 (correctly predicted)
      IF ID EX ME WB    | Instruction 3
         IF ID EX ME WB | Instruction 4
#+end_src
*** Branch prediction miss
#+begin_notes
only at time 4 it realize that it is wrong and need to go to the other branch
so it has to flush
#+end_notes
- example cost 3 cycles but real cpu cost 7-15 cycles
#+begin_src text
Time →      FLUSH
1  2  3  4  5  6  7  8  9 10 11 12 13
IF ID EX ME WB                | Instruction 1 (branch)
   IF ID EX -- -- --          | Instruction 2 (wrong path)
      IF ID -- -- --          | Instruction 3 (wrong path)
         IF -- -- --          | Instruction 4 (wrong path)
            IF ID EX ME WB    | Correct Instruction 2
               IF ID EX ME WB | Correct Instruction 3
#+end_src
*** Arithmetic booleans
#+begin_notes
LLVM optimization is very complex
there are many rules for -o2 -o3
different versions
gcc vs clang llvm
these are just examples
#+end_notes
- actually LLVM does this for you when you do -o2 and -o3
#+ATTR_HTML: :style font-size: 16px;
#+begin_src c++
    // Example 1: Arithmetic with booleans
    bool condition = true;
    int a = 10;
    int b = 20;

    // Branched version
    int x;
    if (condition) {
        x = a;
    } else {
        x = b;
    }
    std::cout << x << std::endl;  // Output: 10

    // Branchless version 1
    x = condition * a + (!condition) * b;
    // Step by step:
    // true * 10 + (!true) * 20
    // 1 * 10 + 0 * 20
    // 10 + 0 = 10
    std::cout << x << std::endl;  // Output: 10

    // Branchless version 2
    x = b + (a - b) * condition;
    // Step by step:
    // 20 + (10 - 20) * true
    // 20 + (-10) * 1
    // 20 - 10 = 10
    std::cout << x << std::endl;  // Output: 10
#+end_src
*** Selection indexing
- actually LLVM does this for you when you do -o2 and -o3
#+begin_src c++
    // Example 2: Tuple indexing
    bool condition = true;
    int a = 10;
    int b = 20;

    // Branched version
    int x;
    if (condition) {
        x = a;
    } else {
        x = b;
    }
    std::cout << x << std::endl;  // Output: 10

    // Branchless version
    std::array<int, 2> values = {b, a};  // Note: array order is {b, a} to match Python's (b, a)
    x = values[condition];
    // Step by step:
    // {20, 10}[true]
    // {20, 10}[1]     // true converts to 1
    // 10
    std::cout << x << std::endl;  // Output: 10

    return 0;
#+end_src
*** If LLVM does it for you, whats the point?
#+begin_notes
LLVM only good at small cases.
For larger complex patterns like JSON.
you must give it as much information as possible to do it.
if you have an array and if it doesn't know that this is 512 bytes, it cannot do simd optimizations

The authors noticed several patterns in the data.
Exploited them and made all operations into SIMD.
Also by batching operations together like maybe do 1 type of operation over the entire string
We can basically almost use SIMD for the entire parsing instead of small minor optimizations.
#+end_notes
- LLVM does it's best, but it cannot find everything
  - good at small cases
- some larger complex patterns
  - human pattern recognition
  - batching operations you can use simd
** Write branchless code (bitwise operations)
*** Tricky memory layout
#+begin_src text
number = 305,419,896
number << 1 # shift left logical
Number: 305,419,896
Hex: 0x12345678
Physical Memory Layout (lowest bit → highest bit)
   Addr Low                           Addr High
     0x1200                              0x1203
        |                                 |
        v                                 v
Before: 00011110 01101010 00110100 00010010
           ↓↓↓↓↓    ↓↓↓↓↓    ↓↓↓↓↓    ↓↓↓↓↓
After:  00001111 00110101 00010110 00100100
        ↑
        0 enters
Decimal: 610,839,792
Hexadecimal: 0x2468ACF0
#+end_src
#+begin_notes
need to know some low level operations to explain all the SIMD things later
#+end_notes
*** Masking
#+begin_src
a = 00001111
b = 11111100

and_op = a & b
and_op = 00001100

 or_op = a | b
 or_op = 11111111

xor_or = a ^ b
xor_or = 11110011
#+end_src
*** Unset right most bit(blsr)
#+begin_src
s = s & (s-1)

a =     00101100
b =     (a - 1)
a =     00101100
b =     00101011
a & b = 00101000
// rightmost bit is unset
#+end_src
- common cpu operation, compiler optimize to ~blsr~
** LLVM Compiler
#+begin_notes
One of the things to do to write fast code is know how the LLVM compiler optimizes your code
There are many many optimizations available for us to use, so I will not go through them,
I'll just talk abit about how optimizations even work in the first place.
#+end_notes
[[https://llvm.org/img/LLVMWyvernSmall.png]]
*** LLVM
#+begin_notes
LLVM is split up into 3 parts,
Frontend, middle-end and backend

the front end will read our c++ source code and output something called a Intermediate Representation

Then the IR will be optimized then the backend will target different cpu platforms like X86, ARM and PowerPC

Thats all LLVM is, its not that difficult
#+end_notes
#+begin_src text
      Frontend         Middle-end              Backend
             ↓                ↓              ↙         ↘
Source Code → LLVM IR → [Optimized IR] → [Assembly] → Machine Code
                                     ↘________________↗
                                     (direct path option)
#+end_src
*** Without LLVM IR
#+begin_notes
Why do we need to have this IR
Every new language we add we need to write compilers to target all the outputs
#+end_notes
#+begin_src text
Without LLVM IR (n*m: 3 languages × 3 targets = 9 compilers)
---------------------------------------------------------
C++   ----→  x86_64
      \---→  AMD
       \--→  ARM

Rust  ----→  x86_64
      \---→  AMD
       \--→  ARM

Haskell --→  x86_64
        \-→  AMD
         \→  ARM

Each arrow represents a separate compiler frontend+backend (9 total)
#+end_src
*** With LLVM IR
#+begin_notes
We just write one compiler to target the IR then it can just generate the output for each architecture
Any optimizations and improvements to the compiler on the right side, we get it for free on the left side.
#+end_notes
#+begin_src text
With LLVM IR (n+m: 3 frontends + 3 backends = 6 components)
--------------------------------------------------------

            ╭→ x86_64
C++    ╮    │
       ↓    │
Rust   ━━→ IR ━━→ AMD
       ↑    │
Haskell╯    │
            ╰→ ARM

            ┊
            ↓
    Shared Optimizations
    - Dead code elimination
    - Loop vectorization
    - Constant propagation
    - And many more...
#+end_src
*** Intermediate Representation Example(IR)
#+begin_notes
Lets talk about an example code here very simple for loop
sum += i*4
return
#+end_notes
#+begin_src c++
int example2(int n) {
    int sum = 0;
    for (int i = 0; i < n; i++) {
        sum += i * 4;  // Multiplication in loop
    }
    return sum;
}
#+end_src
*** Unoptimized IR -O0
#+begin_notes
If we compile with -O0, optimization level 0, no optimization
This is the IR
its this cpu agnostic code that has as much information retained from the original source code as possible
this is because to optimize, the compiler has to know what data types its dealing with
and then it can draw a computation graph to eliminate useless computation
#+end_notes
#+begin_src llvm-ts
define dso_local i32 @_Z8example2i(i32 %0) {
entry:
  %n = alloca i32, align 4
  %sum = alloca i32, align 4
  %i = alloca i32, align 4
  store i32 %0, ptr %n, align 4
  store i32 0, ptr %sum, align 4
  store i32 0, ptr %i, align 4
  br label %for.cond

for.cond:
  %1 = load i32, ptr %i, align 4
  %2 = load i32, ptr %n, align 4
  %cmp = icmp slt i32 %1, %2
  br i1 %cmp, label %for.body, label %for.end

for.body:
  %3 = load i32, ptr %i, align 4
  %mul = mul nsw i32 %3, 4
  %4 = load i32, ptr %sum, align 4
  %add = add nsw i32 %4, %mul
  store i32 %add, ptr %sum, align 4
  br label %for.inc

for.inc:
  %5 = load i32, ptr %i, align 4
  %inc = add nsw i32 %5, 1
  store i32 %inc, ptr %i, align 4
  br label %for.cond

for.end:
  %6 = load i32, ptr %sum, align 4
  ret i32 %6
}
#+end_src
*** Unoptimized IR -O0 Graph
#+begin_notes
this is the computation graph of the IR in front
#+end_notes
#+begin_src mermaid :file attachments/unoptimized-ir.png
flowchart TD
    classDef memop fill:#f9f,stroke:#333
    classDef arithop fill:#afd,stroke:#333
    classDef control fill:#fda,stroke:#333

    param["%0 param"]

    subgraph entry
        alloc_n["%n = alloca"]:::memop
        alloc_sum["%sum = alloca"]:::memop
        alloc_i["%i = alloca"]:::memop
        store_n["store %0 to %n"]:::memop
        store_sum0["store 0 to %sum"]:::memop
        store_i0["store 0 to %i"]:::memop
    end

    subgraph for_cond
        load_i1["load from %i"]:::memop
        load_n["load from %n"]:::memop
        cmp["icmp slt"]:::arithop
        branch_cond["br i1"]:::control
    end

    subgraph for_body
        load_i2["load from %i"]:::memop
        mul["mul * 4"]:::arithop
        load_sum["load from %sum"]:::memop
        add["add"]:::arithop
        store_sum["store to %sum"]:::memop
    end

    subgraph for_inc
        load_i3["load from %i"]:::memop
        inc["add + 1"]:::arithop
        store_i["store to %i"]:::memop
    end

    subgraph for_end
        load_sum_final["load from %sum"]:::memop
        ret["return"]:::control
    end

    param --> store_n
    alloc_n --> store_n
    alloc_sum --> store_sum0
    alloc_i --> store_i0

    store_i0 --> load_i1
    store_n --> load_n
    load_i1 --> cmp
    load_n --> cmp
    cmp --> branch_cond
    branch_cond -->|"i < n"| load_i2
    branch_cond -->|"i >= n"| load_sum_final

    load_i2 --> mul
    mul --> add
    load_sum --> add
    add --> store_sum
    store_sum --> load_i3

    load_i3 --> inc
    inc --> store_i
    store_i --> load_i1

    load_sum_final --> ret
#+end_src

#+RESULTS:
[[file:attachments/unoptimized-ir.png]]

*** Optimized IR -O2
#+begin_notes
this is compiled with -O2
#+end_notes
#+begin_src llvm-ts
define dso_local i32 @_Z8example2i(i32 %0) local_unnamed_addr #0 {
entry:
  %cmp6 = icmp sgt i32 %0, 0
  br i1 %cmp6, label %for.body.preheader, label %for.end

for.body.preheader:
  %1 = add i32 %0, -1
  %2 = mul i32 %0, %1
  %3 = lshr i32 %2, 1
  %4 = mul i32 %3, 4
  br label %for.end

for.end:
  %sum.0.lcssa = phi i32 [ 0, %entry ], [ %4, %for.body.preheader ]
  ret i32 %sum.0.lcssa
}
#+end_src

*** Optimized IR -O2 Graph
#+begin_notes
with deadcode eliminated
#+end_notes
#+begin_src mermaid :file attachments/optimized-ir.png
flowchart TD
    classDef arithop fill:#afd,stroke:#333
    classDef control fill:#fda,stroke:#333

    param["%0 param"]

    subgraph entry
        cmp["icmp sgt i32 %0, 0"]:::arithop
        branch["br i1"]:::control
    end

    subgraph for_body_preheader
        sub["add i32 %0, -1"]:::arithop
        mul1["mul i32 %0, %1"]:::arithop
        shift["lshr i32 %2, 1"]:::arithop
        mul2["mul i32 %3, 4"]:::arithop
    end

    subgraph for_end
        phi["phi i32 [0, entry], [%4, preheader]"]:::control
        ret["ret i32"]:::control
    end

    param --> cmp
    cmp --> branch
    branch -->|"> 0"| sub
    branch -->|"<= 0"| phi

    sub --> mul1
    param --> mul1
    mul1 --> shift
    shift --> mul2
    mul2 --> phi

    phi --> ret

    style param fill:#ddd
    style ret fill:#f96
#+end_src

#+RESULTS:
[[file:attachments/optimized-ir.png]]

* Simdjson Implementation
** Simdjson Architecture Overview
#+begin_notes
indexing structure, basically to find the places where the data is at
object, array, strings
validate UTF-8
1. Stage 1: Structural Index Creation (find location of important markers)
   1. Find structural characters ({,},[,],",,:)
   2. Locate whitespace
   3. Identify string boundaries
   4. Validate UTF-8 encoding
generating output tape to navigate
parsing
1. Stage 2: Parsing & Tape Building
   1. Validate document structure
   2. Build navigable tape representation
      1. Parse atomic values (strings, numbers, true/false/null)
      2. Convert numbers to machine formats
      3. Normalize strings to UTF-8
#+end_notes
1. Stage 1: Structural Index Creation (find location of important markers)
   1. Find structural characters ({,},[,],",,:)
   2. Locate whitespace
   3. Identify string boundaries
   4. Validate UTF-8 encoding
2. Stage 2: Parsing & Tape Building
   1. Validate document structure
   2. Build navigable tape representation
      1. Parse atomic values (strings, numbers, true/false/null)
      2. Convert numbers to machine formats
      3. Normalize strings to UTF-8
** simdjson diagram
#+begin_notes
the actual way it is processed is
reading in the input string
convert to bytes
code interface is working on 512 byte chunks
if there are smaller divisions, it will split it up interally inside and work on those

find the important locations called structural locations as a bitmask

bitmask convert to index

parse the true false null

output the final tape
#+end_notes
#+begin_src text
    JSON INPUT STRING
   "{"name": "value"}"
            ⬇
     512-BYTE CHUNKS -> 512x1,256x2,128x4
   ╔═════════════════╗
   ║"{"name": "val...║
   ╚═════════════════╝
            ⬇
         STAGE 1
  (Bitmap Generation &   find: ([, {, ], }, :, ,)
   Index Extraction)     escaped characters and quoted regions
            ⬇            Validate UTF-8
       INDEX ARRAY
      [0,3,5,7,...]
            ⬇
         STAGE 2         parse number, int, float, 1e10, true, false, null, string
   (Parse & Build Tape)  build tape to navigate
            ⬇
       FINAL TAPE
[.........................]
#+end_src
** Stage 1: Structural and Pseudo Structural Index Construction
*** Input and Output
- Input: Raw JSON bytes
- Output:
  - Bitmask of structural chars
  - Array of integer indices marking structural elements
*** Key Responsibilities
1. Character encoding validation (UTF-8)
2. Locate structural characters ([, {, ], }, :, ,)
3. Identify string boundaries
   1. Handles escaped characters and quoted regions
4. Find pseudo-structural characters (atoms like numbers, true, false, null)

** Stage 2: Structured Navigation
*** Input and Output
- Input: Array of structural indices from Stage 1
- Output: Parsed JSON structure on a "tape"(array)
- Purpose: Build navigable representation of JSON document

*** Key Responsibilities
1. Parse strings and convert to UTF-8
2. Convert numbers to 64-bit integers or doubles
3. Validate structural rules (matching braces, proper sequences)
4. Build navigable tape structure

*** The Tape Format
- 64-bit words for each node
- Special encoding for different types:
  - Atoms (null, true, false): n/t/f × 2^56
  - Numbers: Two 64-bit words
  - Arrays/Objects: Start/end markers with navigation pointers
  - Strings: Pointer to string buffer

* Stage 1: Structural and Pseudo Structural Index Construction
#+begin_notes
this is how the high level overview of this stage, the c++ code looks like
the important thing is this _mm256_shuffle_epi8 == VPSHUFB
it is basically a small nibble level lookup table with 16 slots
i will talk more about this later
#+end_notes
_mm256_shuffle_epi8 == VPSHUFB
#+ATTR_HTML: :style font-size: 16px;
#+begin_src cpp
  // _mm256_shuffle_epi8 == VPSHUFB
  const auto whitespace_table = simd8<uint8_t>::repeat_16(' ', 100, 100, 100, 17, 100, 113, 2, 100, '\t', '\n', 112, 100, '\r', 100, 100);

  const auto op_table = simd8<uint8_t>::repeat_16(
    0, 0, 0, 0,
    0, 0, 0, 0,
    0, 0, ':', '{', // : = 3A, [ = 5B, { = 7B
    ',', '}', 0, 0  // , = 2C, ] = 5D, } = 7D
  );

  const uint64_t whitespace = in.eq({
    _mm256_shuffle_epi8(whitespace_table, in.chunks[0]),
    _mm256_shuffle_epi8(whitespace_table, in.chunks[1])
  });
  // Turn [ and ] into { and }
  const simd8x64<uint8_t> curlified{
    in.chunks[0] | 0x20,
    in.chunks[1] | 0x20
  };
  const uint64_t op = curlified.eq({
    _mm256_shuffle_epi8(op_table, in.chunks[0]),
    _mm256_shuffle_epi8(op_table, in.chunks[1])
  });

  return { whitespace, op };
#+end_src
** Stage 1: 1 Vectorized Classification and Pseudo-Structural Characters
#+begin_notes
we basically looking for these locations these are important loctains so that we can parse in stage 2
notice that the escaped " are not selected
this part will talk about how the authors did it in simd
the trick is to use a look up table
#+end_notes
- Want to obtain location of structural characters  ({, }, [, ], :, ,)
  - pseudo-structural - Any non‐whitespace character that immediately follows a structural character or whitespace
  - useful for parsing, we need this bit mask to build tape
#+begin_src text
{ "\\\"Nam[{": [ 116,"\\\\" , 234, "true", false ], "t":"\\\"" }
__1______________1___1________1____1_______1________1___1_______
______0_____________________________________________________0___ escaped quotes "
#+end_src
*** Vectorized Classification
#+begin_notes
We need to classify structural characters
each different class gets its own type
comma
colon
brackets, array group 4
whitespace
we need to do this classification fast we will use a look up table to do the classification, basically O(1)
notice they have only 1 bit at differnt locations
we then use movemask to split them up into bitmasks

vpshufb is done for the low 4 nibble so we can find that this is a
c a b d b d
high 4 nibble 2 3 5 5 7 7

then we do bitwise and
if we know both bytes then we can shift it to the correct
#+end_notes
#+ATTR_HTML: :style font-size: 50%;
| code points | character   | desired value |   bin |
|-------------+-------------+---------------+-------|
|        0x2c | `,` (comma) |             1 | 00001 |
|        0x3a | `:` (colon) |             2 | 00010 |
|        0x5b | `[`         |             4 | 00100 |
|        0x5d | `]`         |             4 | 00100 |
|        0x7b | `{`         |             4 | 00100 |
|        0x7d | `}`         |             4 | 00100 |
|        0x09 | TAB         |             8 | 01000 |
|        0x0a | LF          |             8 | 01000 |
|        0x0d | CR          |             8 | 01000 |
|        0x20 | SPACE       |            16 | 10000 |
|      others | any other   |             0 | 00000 |
#+begin_src text
HIGH_4 AND LOW_4 == 0000 0100 // it must be a bracket

{ "\\\"Nam[{": [ 116,"\\\\" , 234, "true", false ], "t":"\\\"" }
____________________1_______1____________1________1_____________ comma mask
_____________1_________________________________________1________ colon mask
1_________11___1_________________________________1_____________1 bracket mask
#+end_src
*** VPSHUFB: Vector Permute Shuffle Bytes
#+begin_notes
If you have any experience with hashmaps, they are actually very slow
they are not truely o(1) lookup
the only true O(1) lookup structures are actually arrays, index + offset
hashing function is a fake O(1)

if the first bit is a 0, it will look at the lower 4 nibble
like an index into an array
lets take 0x01 for example, it will go into the table here, look for the item in the first index
and put it in that location
so we get '1'
if the high bit is 1, then it will be null byte 0x00

we can classify 64 chars in 3-4 instructions now
that is just 4 cycles compared to a big if else
#+end_notes
- basically a one instruction lookup table using the 4 lowest bit(nibble)
  - 0000 XXXX
#+begin_src c++
int main() {
    // Lookup table for hex digits "0123456789abcdef"
    __m256i lut = _mm256_setr_epi8(
        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f'
    );

    // Example 2: Alternating normal/zeroed values (0x00,0x80,0x01,0x81...)
    __m256i indices2 = _mm256_setr_epi8(
        0x00, 0x80, 0x01, 0x81, 0x02, 0x82, 0x03, 0x83, 0x04, 0x84, 0x05, 0x85, 0x06, 0x86, 0x07, 0x87,
        0x08, 0x88, 0x09, 0x89, 0x0A, 0x8A, 0x0B, 0x8B, 0x0C, 0x8C, 0x0D, 0x8D, 0x0E, 0x8E, 0x0F, 0x8F
    );

    printf("\nAlternating with zeroes (. represents zero):\n");
    print_bytes(_mm256_shuffle_epi8(lut, indices2));
    // Alternating with zeroes (. represents zero):
    // 0.1.2.3.4.5.6.7.8.9.a.b.c.d.e.f.

    return 0;
}

#pragma GCC target("avx2")
#include <immintrin.h>
#include <stdio.h>
void print_bytes(__m256i v) {
    unsigned char bytes[32];
    _mm256_storeu_si256((__m256i*)bytes, v);
    for(int i = 0; i < 32; i++) {
        if (bytes[i]) {
            printf("%c", bytes[i]);
        } else {
            printf(".");  // Print dot for zero bytes
        }
    }
    printf("\n");
}
#+end_src
*** Simple example
#+begin_notes
skip if no time
vpshufb is done for the low 4 nibble and high 4 nibble
if we know both bytes then we can shift it to the correct
#+end_notes
| code points | character   | desired value |   bin |
|        0x3a | `:` (colon) |             2 | 00010 |
|        0x0a | LF          |             8 | 01000 |
- use vpshufb to match low nibble a
- could be both : and LF so it must match 0010 | 1000 = 1010
- low nibble at position A = 10
  - high nibble 0x3 vs 0x0
    - 0x3 = 2
    - 0x0 = 8
*** Simple example
#+begin_example
"LF:"

Low nibble table
00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15
xx xx xx xx xx xx xx xx xx xx 10 xx xx xx xx xx
1010

high nibble table
00 .. 02 03 04 05 06 07 08 09 10 11 12 13 14 15
08 .. 02 xx xx xx xx xx xx xx xx xx xx xx xx xx
0100,  0010
#+end_example
*** Simple example
|     |      |   LF |    : |
|     | low  | 1010 | 1010 |
|     | high | 1000 | 0010 |
| AND |      | 1000 | 0010 |
|     |      |    8 |    2 |
*** Stage 1: Bitmap to Array index
#+begin_notes
In stage 1, we our functions take in 64 byte * 8 bit blocks

however these masks are sparse, sometimes it can be 4 char before we a faced with a 1
sometimes the spaces are 4, and the spaces could be 40.

if we iterate through this and process it with if else statements, its unpredictable branching and will cause performance penalty, mson does this

as such we want to extract the bits into a list of indexes instead of working directly with the bitsets.
#+end_notes

#+begin_src text
{ "\\\"Nam[{": [ 116,"\\\\" , 234, "true", false ], "t":"\\\"" }: input data
__1_________1________1____1________1____1___________1_1_1____1__: Q
1_________11_1_1____1_______1____1_______1_______11____1_______1: S
_1____________1_1__________1_1____1_______1_____1__1__________1_: W
#+end_src
- take Q for example, we want to convert Q's bit mask into a list of indexes
  - [2, 12, 22, 27, 37, 42, 54, 56, 58, 62]
*** extraction
#+begin_notes
compiler will automatically optimize this for you
#+end_notes
- 2 instructions
  - TZCNT count trailing least significant 0 bits
  - BLSR which delete the last bit.
#+begin_src text
a = 1010000
idx = tzcnt(a) // 4       count 0 after lowest bit
a = blsr(a)    // 1000000 remove lowest set bit
idx = tzcnt(a) // 6       count 0 after lowest bit
[4, 6]
#+end_src
*** Naive Implementation
#+begin_notes
the compiler will automatically optimize this into the tzcnt and blsr
that while loop is the part with the unpredictable branching which will cost 10-20 cycles for every wrong prediction
how do we solve it?
#+end_notes
#+begin_src c++
void extract_set_bits_unoptimized(uint64_t bitset, uint32_t* output) {
    uint32_t pos = 0;

    // This while loop is the source of unpredictable branches
    while (bitset) {
        // Find position of lowest set bit
        uint32_t bit_pos = __builtin_ctzll(bitset);
        // Store the position
        *output++ = bit_pos;
        // Clear the lowest set bit
        bitset &= (bitset - 1);
    }
}
#+end_src
*** Minimal branching implementation
#+begin_notes
unfortunately in this case it cannot avoid the if else check
so the trick we do is that we do 8 times
in one branch check
#+end_notes
#+ATTR_HTML: :style font-size: 16px; width: 95%;
#+begin_src c++
void extract_set_bits_optimized(uint64_t bitset, uint32_t* output) {
    // Get total number of set bits
    uint32_t count = __builtin_popcountll(bitset);
    uint32_t* next_base = output + count;

    // Process 8 bits at a time unconditionally
    while (bitset) {
        // Extract next 8 set bit positions, even if we don't have 8 bits
        *output++ = __builtin_ctzll(bitset);
        bitset &= (bitset - 1);  // Clear lowest set bit (blsr instruction)

        *output++ = __builtin_ctzll(bitset);
        bitset &= (bitset - 1);

        *output++ = __builtin_ctzll(bitset);
        bitset &= (bitset - 1);

        *output++ = __builtin_ctzll(bitset);
        bitset &= (bitset - 1);

        *output++ = __builtin_ctzll(bitset);
        bitset &= (bitset - 1);

        *output++ = __builtin_ctzll(bitset);
        bitset &= (bitset - 1);

        *output++ = __builtin_ctzll(bitset);
        bitset &= (bitset - 1);

        *output++ = __builtin_ctzll(bitset);
        bitset &= (bitset - 1);
    }

    // Reset output pointer to actual end based on real count
    output = next_base;
}
#+end_src
** Stage 1: 2 Eliminated escaped or quoted substring
*** Get backslash
#+begin_notes
next we have to get the quotes
the biggest problem is that we have to escape invalid quotes
this part is very logic intensive and may not make sense at first
i will just quickly go through this without explaining too much
I think you can read the paper to understand this part instead
#+end_notes
#+ATTR_HTML: :style font-size: 16px; width: 120%; margin-left: -10%;
#+begin_src text
{ "\\\"Nam[{": [ 116,"\\\\" , 234, "true", false ], "t":"\\\"" }: input data
___111________________1111_______________________________111____: B = backslash_bits
____111_________________1111______________________________111___: bits_shifted_left = backslash_bits << 1

___111________________1111_______________________________111____: bits
____000_________________0000______________________________000___: inverted = ~bits_shifted_left
___1__________________1__________________________________1______: S = starts = bits & inverted
// we get the first backslash of every group
#+end_src
*** Get odd length sequences starting on an odd offset
#+ATTR_HTML: :style font-size: 14px;
#+begin_src text
{ "\\\"Nam[{": [ 116,"\\\\" , 234, "true", false ], "t":"\\\"" }: input data
_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1: O (constant)
___111________________1111_______________________________111____: B = backslash_bits
___1__________________1__________________________________1______: S = starts = bits & inverted
_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1: O (constant)
___1_____________________________________________________1______: OS = S & O

// add B to OS, yielding carries on backslash sequences with odd starts
___1_____________________________________________________1______: OS = S & O
___111________________1111_______________________________111____: B = backslash_bits
   -->                                                   -->
______1_______________1111__________________________________1___: OC = B + OS

// filter out the backslashes from the previous addition, getting carries only
___111________________1111_______________________________111____: B = backslash_bits
___000________________0000_______________________________000____: ~B
______1_______________1111__________________________________1___: OC = B + OS
______1_____________________________________________________1___: OCO = OC & ~B

// get the odd-length sequence starting on an odd offset and ending on even offset
______1_____________________________________________________1___: OCO = OC & ~B
1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1: E (constant)
______1_____________________________________________________1___: OD2 = OCO & E
{ "\\\"Nam[{": [ 116,"\\\\" , 234, "true", false ], "t":"\\\"" }: input data
// this shows two odd-length sequence starting on an odd offset
#+end_src
*** Get odd length sequences starting on an even offset
its just the reverse of what we done just now
#+ATTR_HTML: :style width: 120%; margin-left: -10%;
#+begin_src text
{ "\\\"Nam[{": [ 116,"\\\\" , 234, "true", false ], "t":"\\\"" }: input data
1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_: E (constant)
___1__________________1__________________________________1______: S = starts = bits & inverted
______________________1_________________________________________: ES = S & E
___111________________1111_______________________________111____: B = backslash_bits
// add B to ES, yielding carries on backslash sequences with even starts
                      --->
___111____________________1______________________________111____: EC = B + ES
// filter out the backslashes from the previous addition, getting carries only
__________________________1_____________________________________: ECE = EC & ~B
// select only the end of sequences ending on an odd offset
__________________________1_____________________________________: ECE = EC & ~B
_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1: O (constant)
________________________________________________________________: OD1 = ECE & ~E
// there are no odd-length sequences of backslashes starting on an even offset
#+end_src
*** Get sequences with odd offset
#+ATTR_HTML: :style width: 110%; margin-left: -5%;
#+begin_src text
// merge results, yielding ends of all odd-length sequence of backslashes
________________________________________________________________: OD1 = ECE & ~E
______1_____________________________________________________1___: OD2 = OCO & E

______1_____________________________________________________1___: OD = OD1 | OD2
{ "\\\"Nam[{": [ 116,"\\\\" , 234, "true", false ], "t":"\\\"" }: input data

// these " are escaped and thus are counted as text instead of structural characters
#+end_src
*** Eliminated escape
#+begin_notes
this is the last step so we remove invalid quotes that are backslashed
notice the CLMUL instruction below, we will talk about that one
how to convert the start and end locations into a continuous mask
#+end_notes
#+begin_src text
{ "\\\"Nam[{": [ 116,"\\\\" , 234, "true", false ], "t":"\\\"" }: input data
__1___1_____1________1____1________1____1___________1_1_1___11__: Q = quotes
______1_____________________________________________________1___: OD
// we remove the escaped " quotes
__1_________1________1____1________1____1___________1_1_1____1__: Q &= ~OD
__1111111111_________11111_________11111____________11__11111___: CLMUL(Q,~0)
#+end_src
*** Get location between quotes mask
#+begin_notes
its basically this operation
we can use this to get the area between the quates
#+end_notes
#+ATTR_HTML: :style font-size: 18px;
#+begin_src c++
uint64_t xorShiftOperations(uint64_t num, bool rightShift, bool tutorial = false) {
    uint64_t result = num;
    for (int i = 1; i <= 64; i++) {
        result ^= (num << i);
    }
    // 0000000000000000000010000000000000000000000000000000000000000000
    // after apply
    // 0000000000000000000011111111111111111111111111111111111111111111

    // 0001000000000000000000000000000000000000000000000000000000000000
    // after apply
    // 0001111111111111111111111111111111111111111111111111111111111111
    return result;
}
#+end_src
*** Sweeping
#+begin_src text
// 0000000000000000000010000000000000000000000000000000000000000000
// OR
// 0001000000000000000000000000000000000000000000000000000000000000
// result
// 0001000000000000000010000000000000000000000000000000000000000000

// 0000000000000000000011111111111111111111111111111111111111111111
// XOR
// 0001111111111111111111111111111111111111111111111111111111111111
// result
// 0001111111111111111100000000000000000000000000000000000000000000
#+end_src
*** Sweeping
#+begin_src text
Final result:
0x00      00111111 11110000 00000111 11000000 00011111 00000000 00001100 11111000       0x07
Initial number:
0x00      00100000 00001000 00000100 00100000 00010000 10000000 00001010 10000100       0x07
After left shift by 1:
0x00      00110000 00001100 00000110 00110000 00011000 11000000 00001111 11000110       0x07
After left shift by 2:
0x00      00111100 00001111 00000111 10111100 00011110 11110000 00001100 00110111       0x07
After left shift by 4:
0x00      00111111 11001111 11110111 11000111 11011111 00011111 00001100 11110100       0x07
After left shift by 8:
0x00      00111111 11110000 00111000 00110000 00011000 11000000 00010011 11111000       0x07
After left shift by 16:
0x00      00111111 11110000 00000111 11000000 00100000 11110000 00001011 00111000       0x07
After left shift by 32:
0x00      00111111 11110000 00000111 11000000 00011111 00000000 00001100 11111000       0x07
#+end_src
*** Sweeping implemented by CLMUL, pclmulqdq
#+begin_notes
Why CLMUL is equivalent to the operation above?
XOR is actually the (a + b) % 2
XOR is addition without the carrying bit
then we multiply because shifting left or right is a multiply
#+end_notes
- Carry Less Multiply
- CLMUL(4, 15)
- 4 * 15
#+begin_src text
        4
X      15
----------
        4
X(8+4+2+1)
----------
        4
        8
       16
+      32
----------
       60
----------
#+end_src
*** Sweeping implemented by CLMUL, pclmulqdq
- CLMUL(4, 15)
- XOR ~= ADD
#+begin_src text
         0100  (4)
   X     1111  (15)
-------------
        00100  (X1 means 4 << 0)
XOR    00100_  (X2 means 4 << 1)
XOR   00100__  (X4 means 4 << 2)
XOR  00100___  (X8 means 4 << 3)
-------------
       111100  (all XORed together)
-------------
#+end_src
*** finally get quote mask
#+begin_src text
{ "\\\"Nam[{": [ 116,"\\\\" , 234, "true", false ], "t":"\\\"" }: input data
__1111111111_________11111_________11111____________11__11111___: CLMUL(Q,~0)
#+end_src
** Stage 1: 3 Character-Encoding Validation
#+begin_notes
next we have to validate the UTF-8
- most data is ascii, first thing to do is check if the first bit of every byte is 0
- if it is, its all validated
- if not we go through the algorithm that the authors wrote
#+end_notes
1. Initial ASCII Fast Path, first bit == 0
2. Main algorithm
   1. Range check(0xF4 saturated subtract)
   2. Continuation Byte validation
*** Check for Ascii fast path
#+begin_src text
Single byte (ASCII):
0xxxxxxx                     (values 0-127)
Values start with 0, remaining 7 bits for data
#+end_src
*** Continuation Byte validation
#+begin_notes
i had showed this just now so we have to detect this pattern
if it is 110 then the next must be 10 to be valid
if its 00 then its nat valid lets see how they
implement this in simd
#+end_notes
#+begin_src text
Single byte (ASCII):
0xxxxxxx                     (values 0-127)
Values start with 0, remaining 7 bits for data

Two bytes:
110xxxxx 10xxxxxx           (values 128-2047)
First byte starts with 110

Three bytes:
1110xxxx 10xxxxxx 10xxxxxx  (values 2048-65535)
First byte starts with 1110

Four bytes:
1111xxxx 10xxxxxx 10xxxxxx 10xxxxxx   (values 65536+)
First byte starts with 11110
#+end_src
*** map to values (VPSHUFB again!)
#+begin_notes
we look at the high nibble for this since only the high nibble is involved
if it is ascii, we use the lookup table to get it to 1
if it is 10xx we set it to 0
if 1100 2
1110 3
1111 4

look at the example below, notice a pattern?
it will be 4000 if it is length 4, length 3 is 300
#+end_notes

| high | Dec |   | high | Dec |
|------+-----+---+------+-----|
| 0000 |   1 |   | 1000 |   0 |
| 0001 |   1 |   | 1001 |   0 |
| 0010 |   1 |   | 1010 |   0 |
| 0011 |   1 |   | 1011 |   0 |
| 0100 |   1 |   | 1100 |   2 |
| 0101 |   1 |   | 1101 |   2 |
| 0110 |   1 |   | 1110 |   3 |
| 0111 |   1 |   | 1111 |   4 |

#+begin_src text
1111xxxx 10xxxxxx 10xxxxxx 10xxxxxx   (values 65536+)
4 0 0 0

1110xxxx 10xxxxxx 10xxxxxx  (values 2048-65535)
3 0 0
#+end_src
*** SIMD validation algorithm
#+begin_notes
this is the algorithm, we shift right-1 add back
shift 2 minus 2 add back
notice we get a 4 3 2 1 3 2 1 1 1 1
at the end of it, there should be nothing bigger than 4
and no 0 if have 0 its wrong
next show example of a wrong one
#+end_notes
#+begin_src text
4 0 0 0 3 0 0 2 0 1 1 1
  4 0 0 0 3 0 0 2 0 1 1 1 // <<= 1 byte, shift left by 1 byte
  3 0 0 0 2 0 0 1 0 0 0 0 // saturated subtract 1 from each byte

4 0 0 0 3 0 0 2 0 1 1 1
  3 0 0 0 2 0 0 1 0 0 0 0
4 3 0 0 3 2 0 2 1 1 1 1   // add it back into the original mapping

4 3 0 0 3 2 0 2 1 1 1 1   // add it back into the original mapping
    4 3 0 0 3 2 0 2 1 1 1 1   // <<= 2 byte, shift left by 2 bytes
    2 1 0 0 1 0 0 0 0 0 0 0   // saturated subtract 2
4 3 2 1 3 2 1 3 1 1 1 1   // add it back
// the end result will have no 0
// none of the numbers are bigger than the original
#+end_src
*** SIMD validation algorithm: Invalid example
#+begin_notes
look at this part, after 4 its supposed to be 4000
but i put a 3 there
lets look at a bad example
we do the same steps shuft left subtract 1
#+end_notes
#+begin_src text
2 0 0 0 4 3 0 0
  2 0 0 0 4 3 0 // shift left 1
  1 0 0 0 3 2 0 // saturated subtract 1
2 1 0 0 4 6 2 0

2 1 0 0 4 6 2 0
    0 0 2 1 0 0 4 6 // shift left 2
    0 0 0 0 0 0 2 4 // saturated subtract 2
2 1 0 0 4 6 4 4

2 0 0 0 4 3 0 0
2 1 0 0 4 6 4 4
    --- zeros found here invalid
          - 6 > 3
#+end_src
* Stage 2: Building the Tape
** Stage 2: The Tape
*** Three Categories of Tape Entries
#+begin_notes
after finding the location of everything and
knowing that it is valid

i will go through how they make this tape and parse the values

a tape is made up of entries, each entry is 64 byte
this is for byte alignment

direct values are the simplest
n must be null, t must be true, f must be false

numbers will take 2 entry 128 bytes

strings take 1 entry and point to another buffer where the real string is at
this allows our stucture to be quickly traversable and not polluted with strings of unknown length

last is the structural entrys
they take 1 entry and the data on the right is the location of the other end of the bracket
in the tape

allows us to skip arrays and objects if we want to
#+end_notes
1. Direct Values (Atoms)
   - null, true, false
   - numbers (integers and floats) - takes 2 tape entries
2. String References
   - Points to separate string buffer
   - Not original JSON string
3. Structural Navigation
   - Array brackets [,]
   - Object braces {,}
   - Contains jump indices
*** Basic Structure
#+begin_notes
so for string and structural
the payload is the offset to the buffer
jump index to the other location of the structure in the tape

else the other one is a type marker
#+end_notes
- Tape is array of 64-bit words
- Each entry: =TYPE_MARKER × 2^56 + payload=
- High 8 bits: Type information
- Low 56 bits: Value or reference
#+BEGIN_SRC text
63      56 55                   0
+--------+----------------------+
|  TYPE  |     PAYLOAD          |
+--------+----------------------+
   8 bits      56 bits
#+END_SRC
*** Direct Values (Atoms)
#+begin_notes
these just tell you the type
n for null
t for true
f for false
#+end_notes
#+BEGIN_SRC text
01101110 00000000 00000000 00000000 00000000 00000000 00000000 00000000
   ^'n' null
Hex: 0x6E00000000000000

01110100 00000000 00000000 00000000 00000000 00000000 00000000 00000000
   ^'t' true
Hex: 0x7400000000000000

01100110 00000000 00000000 00000000 00000000 00000000 00000000 00000000
   ^'f' false
Hex: 0x6600000000000000
#+END_SRC
*** Number: Integer Example (42)
Takes 2 tape entries:
- first one is just a type marker
- second is the value
#+BEGIN_SRC text
Entry 1 (type marker):
01101100 00000000 00000000 00000000 00000000 00000000 00000000 00000000
   ^'l'
Hex: 0x6C00000000000000

Entry 2 (value):
00000000 00000000 00000000 00000000 00000000 00000000 00000000 00101010
                                                                 ^42
Hex: 0x000000000000002A
#+END_SRC
*** Number: Float Example (3.14)
Takes 2 tape entries:
#+BEGIN_SRC text
Entry 1 (type marker):
01100100 00000000 00000000 00000000 00000000 00000000 00000000 00000000
   ^'d'
Hex: 0x6400000000000000

Entry 2 (value in IEEE 754):
01000000 00001001 00011110 10111000 01010100 01000000 00000000 00000000
Hex: 0x4009219940000000
#+END_SRC
*** String Tape Entry
#+BEGIN_SRC text
Example for "..........hello":
Binary:
00100010 00000000 00000000 00000000 00000000 00000000 00000000 00001010
   ^'"'                                                         ^offset=10
Hex: 0x220000000000000A
#+END_SRC
- The string buffer is a separate array that stores normalized UTF-8 strings
**** Benefits of This Approach
- Fast length retrieval - no variable length guessing search in tape
- Contains normalized UTF-8 strings
*** Object Example
#+BEGIN_SRC text
{"name": "John"}

Opening brace (points forward):
Binary:
01111011 00000000 00000000 00000000 00000000 00000000 00000000 00000010
   ^'{'                                                         ^next=2
Hex: 0x7B00000000000002

Closing brace (points backward):
Binary:
01111101 00000000 00000000 00000000 00000000 00000000 00000000 00000000
   ^'}'                                                         ^prev=0
Hex: 0x7D00000000000000
#+END_SRC
*** Array Example
#+begin_notes
see here i can have this index here to jump to the other side without reading the array
next noe is more important
#+end_notes
#+begin_src text
array = [1,2,3]
#+end_src
| addr | type    | char |         tape entry |
|------+---------+------+--------------------|
|    0 | array   | [--8 | 0x5B00000000000008 |
|    1 | integer | l    | 0x6C00000000000000 |
|    2 | value   |      | 0x0000000000000001 |
|    3 | integer | l    | 0x6C00000000000000 |
|    4 | value   |      | 0x0000000000000002 |
|    5 | integer | l    | 0x6C00000000000000 |
|    6 | value   |      | 0x0000000000000003 |
|    7 | array   | ]--0 | 0x5D00000000000000 |
|    8 | other   |      |              other |
*** JSON Document
#+begin_notes
this is just an example of how an object would look like
#+end_notes
#+BEGIN_SRC json
{
  "name": "John",
  "age": 42,
  "active": true
}
#+END_SRC
#+BEGIN_SRC text
Idx  Type    Payload   Description
0:   'r'     12        Root (points to end)
1:   '{'     12        Object start (points to end)
2:   '"'     100       String "name" (points to string buffer offset 100)
3:   '"'     150       String "John" (points to string buffer offset 150)
4:   '"'     200       String "age" (points to string buffer offset 200)
5:   'l'     0         Integer marker
6:   -       42        Integer value
7:   '"'     250       String "active" (points to string buffer offset 250)
8:   't'     0         true value
9:   '}'     1         Object end (points to start)
10:  'r'     0         Root end (points to start)
#+END_SRC
*** Tape benefits
- Cache-friendly linear layout
- Fast navigation with index jumping
- SIMD-friendly processing
- Predictable memory layout
** Stage 2: 1 Number parsing
*** Understanding the is_all_digits
#+begin_notes
when parsing for numbers, there is no choice but to use if else

however the authors noticed many numbers are longer than 8 digits
so they develop a way to check and parse numbers of 8 digit size

this is the algo for fast digit check
we run this exact algorithm and if the output is 3333
it must all be digits, let me explain why
#+end_notes
Fast 8 digit check
#+begin_src c++
uint64 high_nibble = val & 0xF0F0F0F0F0F0F0F0;
uint64 low_nibble = ((val + 0x0606060606060606) & 0xF0F0F0F0F0F0F0F0) >> 4;
uint64 combined = high_nibble | low_nibble;
bool is_all_digits = combined == 0x3333333333333333;
#+end_src
*** Key Insight: ASCII Characters from 0x29 to 0x3A
#+begin_notes
So there are 2 things to check, less than 0x30 which is 0x2F up here
0x3A which is bigger than 9 0x39 down here
see a pattern notice all of these are 3
and notice the one behind is 0123..9
#+end_notes
- notice all high nibble of valid digits are 3
#+ATTR_HTML: :style font-size: 80%;
| Char |  Hex | Binary    | Description   |                        |
|------+------+-----------+---------------+------------------------|
| '/'  | 0x2F | 0010 1111 | Forward Slash |                        |
|------+------+-----------+---------------+------------------------|
| '0'  | 0x30 | 0011 0000 | Digit Zero    | <-- Valid digits start |
| '1'  | 0x31 | 0011 0001 | Digit One     |                        |
| '2'  | 0x32 | 0011 0010 | Digit Two     |                        |
| '3'  | 0x33 | 0011 0011 | Digit Three   |                        |
| '4'  | 0x34 | 0011 0100 | Digit Four    |                        |
| '5'  | 0x35 | 0011 0101 | Digit Five    |                        |
| '6'  | 0x36 | 0011 0110 | Digit Six     |                        |
| '7'  | 0x37 | 0011 0111 | Digit Seven   |                        |
| '8'  | 0x38 | 0011 1000 | Digit Eight   |                        |
| '9'  | 0x39 | 0011 1001 | Digit Nine    | <-- Valid digits end   |
|------+------+-----------+---------------+------------------------|
| ':'  | 0x3A | 0011 1010 | Colon         |                        |
*** Step 1: Initial masking of high nibbles
#+begin_notes
lets go throuh an example first check must be that the
high nibble is all 3
very easy in c++ it looks like this
#+end_notes
#+begin_src c++
uint64 high_nibble = val & 0xF0F0F0F0F0F0F0F0;
#+end_src
- if you are lesser than 0x3X, you are 0x2F,
- Let's take valid input "12345678":
#+BEGIN_EXAMPLE
Input bytes:    31 32 33 34 35 36 37 38
                || || || || || || || ||
                v| v| v| v| v| v| v| v|
High nibble:    3  3  3  3  3  3  3  3
                |  |  |  |  |  |  |  |
Mask:           F0 F0 F0 F0 F0 F0 F0 F0
                =  =  =  =  =  =  =  =
Result1:        30 30 30 30 30 30 30 30
#+END_EXAMPLE
*** How the low nibble check works
#+begin_notes
we want this range to be illegal
we do that with carry bit detection
if we add some number, it will over flow and affect the next bit and cause the number to be bigger than 3
#+end_notes
- we want to ensure that low nibble is within 0xX0 - 0xX9
  - 0xXA - 0xXF is illegal
    - Analyzing Carry Detection with Binary
*** Case 1: Valid Digit (0x39 = '9')
#+begin_notes
they chose 0110 is because 9 + 6 = 15
it is still within

imagine if the value is 10, invalid, + 6 will overflow
#+end_notes
#+BEGIN_EXAMPLE
0x39 = 0011 1001  (Original value '9')
0x06 = 0000 0110  (Value we add)
      -----------
      0011 1111  (Result = 0x3F)
Low nibble does not overflow into high nibble and affect the 0x3 in high nibble

After masking high nibble (& 0xF0):
0x3F = 0011 1111
0xF0 = 1111 0000
      -----------
      0011 0000  (= 0x30)

After right shift by 4:
0x30 >> 4 = 0000 0011  (= 0x03) ✓ Valid!
#+END_EXAMPLE
*** Case 2: Invalid Character (0x3A = ':')
#+BEGIN_EXAMPLE
0x3A = 0011 1010  (Original value ':')
0x06 = 0000 0110  (Value we add)
      -----------
       0011 0000
          1 0000
      -----------
      0100 0000  (Result = 0x40) <- Notice the carry!
                                   The '1' carried into the high nibble

After masking high nibble (& 0xF0):
0x40 = 0100 0000
0xF0 = 1111 0000
      -----------
      0100 0000  (= 0x40)

After right shift by 4:
0x40 >> 4 = 0000 0100  (= 0x04) ✗ Invalid!

 0x3X
|0xX4
-----
 0x34 <- INVALID
-----
#+END_EXAMPLE

*** Step 2: Add 0x06 to detect non-digits
#+BEGIN_EXAMPLE
Low nibbles:    1  2  3  4  5  6  7  8
Add 0x06:       7  8  9  A  B  C  D  E
                ^  ^  ^  ^  ^  ^  ^  ^
                |  |  |  |  |  |  |  |
If original <= 9: No carry to high nibble
If original > 9: Carry affects high nibble
#+END_EXAMPLE

*** Step 3: Example with valid digits (0-9)
#+begin_notes
skip
#+end_notes
Take "12345678":
#+BEGIN_EXAMPLE

Original:       31 32 33 34 35 36 37 38
                v  v  v  v  v  v  v  v
high nibble:    30 30 30 30 30 30 30 30

Original:       31 32 33 34 35 36 37 38
After +0x06:    37 38 39 3A 3B 3C 3D 3E
Mask high:      30 30 30 30 30 30 30 30
low nibble:     03 03 03 03 03 03 03 03

high nibble:    30 30 30 30 30 30 30 30
low nibble:     03 03 03 03 03 03 03 03
OR together:    33 33 33 33 33 33 33 33
#+END_EXAMPLE

*** Step 4: Example with invalid character (';' = 0x3B)
Take "1234;678":
#+BEGIN_EXAMPLE
Original:       31 32 33 34 3B 36 37 38
After +0x06:    37 38 39 3A 41 3C 3D 3E
                               ^
                               |
Mask high:      30 30 30 30 40 30 30 30
                               ^ Different!
Shift right 4:  03 03 03 03 04 03 03 03
high nibble:    30 30 30 30 30 30 30 30
OR together:    33 33 33 33 34 33 33 33 ≠ 0x3333...
                               ^ Caught!
#+END_EXAMPLE

*** Why It Works
#+begin_notes
skip
#+end_notes
1. First part (val & 0xF0F0...):
   - Isolates high nibbles
   - Must be 0x30 for valid digits

2. Second part ((val + 0x06...) & 0xF0...):
   - Adding 0x06 to low nibble:
     - For 0-9: Result stays within nibble
     - For >9: Causes carry
   - After shift right 4:
     - Valid digits: Always 0x03
     - Invalid: Different value

3. When OR'd together:
   - Valid digits: Always 0x33
   - Invalid: Different pattern

*** Valid Cases
#+BEGIN_EXAMPLE
"00000000" -> 0x3333333333333333 ✓
"99999999" -> 0x3333333333333333 ✓
"12345678" -> 0x3333333333333333 ✓
#+END_EXAMPLE

*** Invalid Cases
#+BEGIN_EXAMPLE
"A" (0x41):
Original:  41
+0x06:     47
High:      40 ≠ 30 -> Fails

"/" (0x2F):
Original:  2F
+0x06:     35
High:      20 ≠ 30 -> Fails

":" (0x3A):
Original:  3A
+0x06:     40
High:      40 ≠ 30 -> Fails
#+END_EXAMPLE

*** Performance Benefits
- Single comparison instead of 8 individual checks
- No branches (important for modern CPUs)
- Uses native 64-bit operations
- Exploits CPU's ability to do parallel checks

This algorithm is a beautiful example of bit manipulation that turns what would normally be 8 comparisons into a single mathematical test.
*** Understanding SIMD-Based Fast Eight-Digit Number Parsing
#+begin_notes
after we know that it is all digits,
we need to parse the number into a valid number
we wrote this algo before
the authors did a simd version which looks like this
it looks scary
i will explain each step
notice the weird pattern 10 1 10 1
notice the weird pattern 100 1 100 1
notice the weird pattern 10000 1 10000 1
you may already guess what is going to happen
#+end_notes
Convert ASCII string of 8 digits to integer using SIMD instructions.
Example: "12345678" -> 12345678
#+begin_src c++
uint32_t parse_eight_digits_unrolled(char *chars) {
  __m128i ascii0 = _mm_set1_epi8(’0’);
  __m128i mul_1_10 = _mm_setr_epi8(10, 1, 10, 1, 10, 1, 10, 1, 10, 1, 10, 1, 10, 1, 10, 1);
  __m128i mul_1_100 = _mm_setr_epi16(100, 1, 100, 1, 100, 1, 100, 1);
  __m128i mul_1_10000 = _mm_setr_epi16(10000, 1, 10000, 1, 10000, 1, 10000, 1);
  __m128i number_ascii = _mm_loadu_si128((__m128i *)chars);
  __m128i in = _mm_sub_epi8(number_ascii, ascii0);
  __m128i t1 = _mm_maddubs_epi16(in, mul_1_10);
  __m128i t2 = _mm_madd_epi16(t1, mul_1_100);
  __m128i t3 = _mm_packus_epi32(t2, t2);
  __m128i t4 = _mm_madd_epi16(t3, mul_1_10000);
  return _mm_cvtsi128_si32(t4);
}
#+end_src
*** Step 1: Convert ASCII to Numeric Values
#+begin_notes
remove the high nibble so that they are int8
#+end_notes
#+begin_src c++
  __m128i ascii0 = _mm_set1_epi8(’0’);
  __m128i number_ascii = _mm_loadu_si128((__m128i *)chars);
  __m128i in = _mm_sub_epi8(number_ascii, ascii0);
#+end_src

#+BEGIN_EXAMPLE
Input:          "12345678"
ASCII values:   31 32 33 34 35 36 37 38
Subtract:       30 30 30 30 30 30 30 30
Subtract '0':   01 02 03 04 05 06 07 08  (numeric values)
                |  |  |  |  |  |  |  |
Instruction:    _mm_sub_epi8 (PSUBB - packed subtract bytes)
#+END_EXAMPLE

*** Step 2: Multiply Alternate Digits by 10 and Add
#+begin_notes
notice the weird pattern 10 1 10 1
multiply the one infront by 10
add them to each other
#+end_notes
#+begin_src c++
  __m128i mul_1_10 = _mm_setr_epi8(10, 1, 10, 1, 10, 1, 10, 1, 10, 1, 10, 1, 10, 1, 10, 1);
  __m128i t1 = _mm_maddubs_epi16(in, mul_1_10);
#+end_src
#+BEGIN_EXAMPLE
Values:         1  2  3  4  5  6  7  8
Multipliers:   10  1 10  1 10  1 10  1
                |  |  |  |  |  |  |  |
Results:       10  2 30  4 50  6 70  8
                \ /   \ /   \ /   \ /
Sums:           12    34    56    78     (as 16-bit values)

Instruction: _mm_maddubs_epi16 (PMADDUBSW - multiply and add unsigned bytes to signed words)
#+END_EXAMPLE

*** Step 3: Multiply Alternate 16-bit Values by 100
#+begin_notes
then we do the same thing again
100
next is it 10000?
#+end_notes
#+begin_src c++
  __m128i mul_1_100 = _mm_setr_epi16(100, 1, 100, 1, 100, 1, 100, 1);
  __m128i t2 = _mm_madd_epi16(t1, mul_1_100);
#+end_src
#+BEGIN_EXAMPLE
Values:        12   34   56   78
Multipliers:  100    1  100    1
                |    |    |    |
Results:     1200   34 5600   78
                 \ /       \ /
Sums:           1234      5678    (as 32-bit values)

Instruction: _mm_madd_epi16 (PMADDWD - multiply and add packed words)
#+END_EXAMPLE
- what is the next step? 10000?
#+begin_src c++
  __m128i mul_1_10000 = _mm_setr_epi16(10000, 1, 10000, 1, 10000, 1, 10000, 1);
#+end_src

*** Step 4: Pack 32-bit Values to 16-bit
- reinterpret value as 32 bit instead of 16 bits!? why?
- so we can use ~_mm_setr_epi16~ instead of ~_mm_setr_epi32~
  - its more efficient
#+begin_src c++
    uint16 max_value = 65536;
  __m128i t3 = _mm_packus_epi32(t2, t2);
#+end_src
#+BEGIN_EXAMPLE
Before:   1234(32-bit)  5678(32-bit)
After:    1234(16-bit)  5678(16-bit)

Instruction: _mm_packus_epi32 (PACKUSDW - pack with unsigned saturation)
#+END_EXAMPLE

*** Step 5: Final Combine with Multiply by 10000
#+begin_notes
finally we do the 10000 and get the answer
#+end_notes
#+begin_src c++
  __m128i mul_1_10000 = _mm_setr_epi16(10000, 1, 10000, 1, 10000, 1, 10000, 1);
  __m128i t4 = _mm_madd_epi16(t3, mul_1_10000);
#+end_src
#+BEGIN_EXAMPLE
Values:        1234     5678
Multipliers:  10000        1
                  |        |
Results:   12340000     5678
                   \   /
Sum:           12345678    (final 32-bit result)

Instruction: _mm_madd_epi16 (PMADDWD again)
#+END_EXAMPLE
*** Summary: Why This is Fast
#+begin_notes
simd fast
save instructions
it takes aronud 17 cycles instead of much more
#+end_notes
1. Parallel Processing:
   - Processes multiple digits simultaneously
   - Uses CPU's SIMD capabilities efficiently

2. Instruction Count:
   - Traditional: ~8 loads + ~8 multiplies + ~7 adds ~23 inst
   - SIMD: ~7 total instructions

3. Latency Analysis on Haswell:
   - PSUBB (subtract): 1 cycle
   - PMADDUBSW (multiply-add bytes): 5 cycles
   - PMADDWD (multiply-add words): 5 cycles
   - PACKUSDW (pack): 1 cycle
   - Total latency: ~17 cycles
* Actual c++ code implementation and optimization tricks in the code base
** SIMD8 zero cost "abstraction"
#+begin_notes
actual coding style
how they made their life easier
this is how they made their classes
heavy reuse of templating
#+end_notes
#+ATTR_HTML: :style font-size: 40%; width: 50%
#+begin_src text
                          +---------------------+
                          |    base<Child>      |
                          +---------------------+
                                    │
                                    │
                     +--------------┴--------------+
                     |                             |
              (for T ≠ bool)                   (for bool)
                     |                             |
              +------▼------+                +------▼------+
              |   base8<T>  |                | base8<bool> |
              +-------------+                +-------------+
                     │                             │
                     │                             │
            +--------▼--------+                    │
            | base8_numeric<T>|                    │
            +-----------------+                    │
                     │                       +-----▼-----+
           +---------┴---------+             |simd8<bool>|
           |                   |             +-----------+
           |                   |
   +-------▼-------+   +-------▼-------+
   | simd8<int8_t> |   | simd8<uint8_t>|
   +---------------+   +---------------+

base<simd8<uint8_t>>     // Base template
    ↑
base8<uint8_t>          // Adds common SIMD operations
    ↑
base8_numeric<uint8_t>  // Adds numeric operations
    ↑
simd8<uint8_t>         // Final implementation
#+end_src
*** Quality of life abstractions
#+begin_notes
they overwrote the max min <= >= operators to make using simd intuitive
look here got add substract bla bla
#+end_notes
#+ATTR_HTML: :style width: 140%; margin-left: -20%;
#+begin_src c++
  template<>
  struct simd8<uint8_t>: base8_numeric<uint8_t> {
    // Saturated math
    simdjson_inline simd8<uint8_t> saturating_add(const simd8<uint8_t> other) const { return _mm256_adds_epu8(*this, other); }
    simdjson_inline simd8<uint8_t> saturating_sub(const simd8<uint8_t> other) const { return _mm256_subs_epu8(*this, other); }

    // Order-specific operations
    simdjson_inline simd8<uint8_t> max_val(const simd8<uint8_t> other) const { return _mm256_max_epu8(*this, other); }
    simdjson_inline simd8<uint8_t> min_val(const simd8<uint8_t> other) const { return _mm256_min_epu8(other, *this); }
    // Same as >, but only guarantees true is nonzero (< guarantees true = -1)
    simdjson_inline simd8<uint8_t> gt_bits(const simd8<uint8_t> other) const { return this->saturating_sub(other); }
    // Same as <, but only guarantees true is nonzero (< guarantees true = -1)
    simdjson_inline simd8<uint8_t> lt_bits(const simd8<uint8_t> other) const { return other.saturating_sub(*this); }
    simdjson_inline simd8<bool> operator<=(const simd8<uint8_t> other) const { return other.max_val(*this) == other; }
    simdjson_inline simd8<bool> operator>=(const simd8<uint8_t> other) const { return other.min_val(*this) == other; }
    simdjson_inline simd8<bool> operator>(const simd8<uint8_t> other) const { return this->gt_bits(other).any_bits_set(); }
    simdjson_inline simd8<bool> operator<(const simd8<uint8_t> other) const { return this->lt_bits(other).any_bits_set(); }
  };
#+end_src
*** Quality of life abstractions
#+begin_notes
same thing
#+end_notes
#+ATTR_HTML: :style width: 140%; margin-left: -20%;
#+begin_src c++
    // Bit-specific operations
    simdjson_inline simd8<bool> bits_not_set() const { return *this == uint8_t(0); }
    simdjson_inline simd8<bool> bits_not_set(simd8<uint8_t> bits) const { return (*this & bits).bits_not_set(); }
    simdjson_inline simd8<bool> any_bits_set() const { return ~this->bits_not_set(); }
    simdjson_inline simd8<bool> any_bits_set(simd8<uint8_t> bits) const { return ~this->bits_not_set(bits); }
    simdjson_inline bool is_ascii() const { return _mm256_movemask_epi8(*this) == 0; }
    simdjson_inline bool bits_not_set_anywhere() const { return _mm256_testz_si256(*this, *this); }
    simdjson_inline bool any_bits_set_anywhere() const { return !bits_not_set_anywhere(); }
    simdjson_inline bool bits_not_set_anywhere(simd8<uint8_t> bits) const { return _mm256_testz_si256(*this, bits); }
    simdjson_inline bool any_bits_set_anywhere(simd8<uint8_t> bits) const { return !bits_not_set_anywhere(bits); }
    template<int N>
    simdjson_inline simd8<uint8_t> shr() const { return simd8<uint8_t>(_mm256_srli_epi16(*this, N)) & uint8_t(0xFFu >> N); }
    template<int N>
    simdjson_inline simd8<uint8_t> shl() const { return simd8<uint8_t>(_mm256_slli_epi16(*this, N)) & uint8_t(0xFFu << N); }
    // Get one of the bits and make a bitmask out of it.
    // e.g. value.get_bit<7>() gets the high bit
    template<int N>
    simdjson_inline int get_bit() const { return _mm256_movemask_epi8(_mm256_slli_epi16(*this, 7-N)); }
#+end_src
** Template Metaprogramming & CRTP vs. Virtual Functions (Dynamic Binding)
#+begin_notes
here iis the thing
this template meta programming means using template which is a direct substitution that is known at compile time
this allows the LLVM to aggresively inline code
inlining allows for max information so it can optimize properly
The compiler can inline these functions, reducing function call overhead.
no virtual function
no run time indirection. Since the dispatch is resolved at compile time, there's no need for a vtable lookup.
#+end_notes
- **Compile-Time Polymorphism with Templates/CRTP:**
  - **Zero-Cost Abstraction:** The CRTP pattern lets the compiler resolve function calls at compile time.
    - *Example from simdjson:*
#+BEGIN_SRC c++
  template<typename Child>
  struct base {
    // Overloaded operator (inline, no vtable overhead)
    simdjson_inline Child operator|(const Child other) const {
      return _mm256_or_si256(*this, other);
    }
  };
#+END_SRC
- **Inlining & Optimization**
- **No Runtime Indirection**
*** Dynamic Binding with Virtual Functions
#+begin_notes
they prefer early binding in template rather than virtual functions
virtual functions are like interfaces in java
the reason why java is still fast when doing this is because
java has a JIT, c++ does not
java can optimize code at runtime by observation
while c++ must be optimized at compile time
- **Runtime Overhead:**
  - Each call incurs an extra indirection (vtable lookup).
  - Virtual calls are generally not inlined because the exact function is only known at runtime.
- **Comparable to Java Interfaces:**
  - In Java, interface methods (or virtual methods) are dispatched at runtime. While a JIT can sometimes inline such calls when it detects hot paths, C++ does not have a JIT and relies entirely on compile‑time optimizations.
  - This makes virtual functions in C++ a “costly” alternative when compared to template-based approaches for performance‑critical code.
#+end_notes
- **Late Binding:** Function calls are resolved at runtime via a vtable.
  - *Example (the costly alternative):*
#+BEGIN_SRC c++
  struct Base {
    virtual void foo() = 0;
    virtual ~Base() = default;
  };

  struct Derived : Base {
    void foo() override {
      // ... implementation ...
    }
  };
#+END_SRC
- **Runtime Overhead:**
  - indirection
  - cannot inline
- **Comparable to Java Interfaces:**
*** Why C++ Chooses Compile-Time Polymorphism
#+begin_notes
this makes c++ code fast
deterministic
but cause JIT come in, it might jitter, first slow
fast then still have GC
easier analysis
- **Performance Sensitivity:** In scenarios like high‑performance parsing (as in simdjson), every extra cycle counts.
- **Deterministic Overhead:** With templates/CRTP, the performance characteristics are known at compile time—there’s no hidden cost of runtime dispatch.
- **Contrast with Java:**
  - Java’s JIT can optimize away some of the virtual call overhead during runtime, but C++ has to resolve everything during compilation, making it essential to use techniques that yield zero‑overhead abstractions.
#+end_notes
| Java                                   | C++                                     |
|----------------------------------------+-----------------------------------------|
| • Runtime method dispatch via JIT      | • Compile-time resolution via templates |
| • Variable latency due to GC           | • No GC = predictable latency           |
| • Performance changes during execution | • Performance known at compile time     |
| • Requires "warm up" for optimization  | • Consistent from first call            |
** Inline Functions & Compile-Time Inlining
#+begin_notes
they did inlining to help the compiler get optimal code
#+end_notes
- **Technique:** Functions are marked with `simdjson_inline` to encourage inlining.
- **Why?** Inlining eliminates function call overhead for tiny, frequently used functions.
- **Example from simdjson:**
#+BEGIN_SRC c++
#elif defined(__GNUC__) && !defined(__OPTIMIZE__)
  // If optimizations are disabled, forcing inlining can lead to significant
  // code bloat and high compile times. Don't use simdjson_really_inline for
  // unoptimized builds.
  #define simdjson_inline inline
#else

// Overloaded bitwise OR operator
simdjson_inline Child operator|(const Child other) const {
  return _mm256_or_si256(*this, other);
}
#+END_SRC
- **Note:** The use of inlining on all small operations (e.g. arithmetic, bitwise operators) ensures maximum performance.
** C++ Casts in simdjson: Performance Considerations
#+begin_notes
here we talk about the casting they use
they only use static_cast and reintepret_cast which is also another
0 cost compile time casting
while the other 2 are runtime checked
#+end_notes
- In high‑performance C++ code, using the proper cast is essential for both safety and speed.
- C++ provides several cast operators:
  - **static_cast**: Compile‑time conversions.
  - **reinterpret_cast**: Low‑level, pointer and bit‑reinterpretation.
  - **const_cast**: Remove constness.
  - **dynamic_cast**: Runtime-checked casts (with RTTI).
*** static_cast for CRTP Efficiency
#+begin_notes
for example this static cast is used to make sure the other type is the same
#+end_notes
- known at compile‑time, ensuring zero‑cost abstraction.
#+BEGIN_SRC c++
  template<typename Child>
  struct base {
    __m256i value;
    // Overloaded compound assignment using CRTP
    simdjson_inline Child& operator|=(const Child other) {
      auto this_cast = static_cast<Child*>(this);
      *this_cast = *this_cast | other;
      return *this_cast;
    }
  };
#+END_SRC
- **Notes:**
  - The `static_cast<Child*>(this)` converts the base class pointer to the derived type.
*** reinterpret_cast for SIMD Memory Operations
#+begin_notes
convert array into 256 byte for simd use
- **Why?:** When working with intrinsics (e.g. AVX2), you need to treat data as a special type (like `__m256i`), and reinterpret_cast does this with no extra overhead.
#+end_notes
- Reinterpret raw memory (such as an array of bytes) as SIMD register types.
- cannot static cast, type checked
#+BEGIN_SRC c++
  static simdjson_inline simd8<T> load(const T values[32]) {
    return _mm256_loadu_si256(reinterpret_cast<const __m256i *>(values));
  }
#+END_SRC
- **Notes:**
  - These reinterpret_casts allow the compiler to generate efficient SIMD load/store instructions.
  - They incur no runtime penalty as they are resolved during compilation.
*** Why Not dynamic_cast or const_cast?
- **dynamic_cast:**
  - Performs runtime type checking and incurs additional overhead.
- **const_cast:**
  - const -> other type
*** Summary of Casts in simdjson
- **static_cast:**
  - Used for compile‑time conversions (e.g. CRTP base-to-derived pointer conversion).
  - Zero‑cost and type‑safe.
- **reinterpret_cast:**
  - Used for pointer re‑interpretation (e.g. converting a byte array to a SIMD register pointer).
  - Necessary for interfacing with low‑level intrinsics.
- **Avoided Casts:**
  - **dynamic_cast** and **const_cast** are not used in performance-critical sections to prevent unnecessary runtime overhead.
** Why Error Codes Outperform Exceptions
#+begin_notes
error code does not unwind stack like how except does
more linera flow
predictable branching
#+end_notes
- Zero-cost error handling: No stack unwinding or EH tables
- Better compiler optimizations: Linear control flow
- Predictable branch patterns: CPU pipelining friendly
- Smaller code size: No exception handling metadata
#+begin_src c++
simdjson_warn_unused error_code minify(const uint8_t *buf, size_t len, uint8_t *dst, size_t &dst_len) const noexcept final {
  return set_best()->minify(buf, len, dst, dst_len);
}
#+end_src
*** Assembly Comparison: Error Code Path (simdjson style)
#+begin_notes
error path
#+end_notes
#+begin_src asm
check_ascii:
  vptest %ymm0, %ymm1
  jne .error        ; Single conditional branch
  ; ... normal path ...

.error:             ; simd branchless way if possible
  mov eax, 1        ; Set error code
  ret
#+end_src
*** Assembly Comparison: Exception Path
#+begin_notes
except path show the stack trace there
#+end_notes
#+begin_src asm
check_ascii:
  vptest %ymm0, %ymm1
  jne .exception
  ; ... normal path ...

.exception:
  call __cxa_allocate_exception  ; Heavy EH machinery
  ; ... stack unwinding setup ...
  ; - Exception table lookups
  ; - Destructor calls
  ; - Catch handler matching
  ; - Stack unwinding
#+end_src
*** Key Performance Factors
1. **No EH Table Overhead**
   - Exception handling requires RTTI and stack unwinding tables

2. **CPU Branch Prediction**
   - Error codes use simple conditional branches
     - Exceptions create unpredictable control flow

3. **Inlining Friendly**
   - Error return paths don't inhibit function inlining
   - Critical for SIMD optimizations
** Memory Alignment & Padding
#+begin_notes
- we working with SBE should know about this
  i'll skip its just that aligned access is faster than non aligned access
- simdjson uses types such as padded_string and padded_string_view to guarantee ample padding.
#+end_notes
- Correct memory alignment (and extra padding) is crucial for SIMD operations; unaligned accesses can severely hurt performance.
#+BEGIN_SRC cpp
simdjson::padded_string_view get_padded_string_view(const char *buf, size_t len,
                                                   simdjson::padded_string &jsonbuffer) {
  if (need_allocation(buf, len)) { // unlikely case
    jsonbuffer = simdjson::padded_string(buf, len);
    return jsonbuffer;
  } else { // no allocation needed (most common)
    return simdjson::padded_string_view(buf, len, len + simdjson::SIMDJSON_PADDING);
  }
}
#+END_SRC
** Loop Unrolling and Vectorized Processing
#+begin_notes
loop unrolling is debatable sometimes it helps
sometimes is bad
in this case it helps because we are using fixed 512 byte sizes
do not unroll things if you dont know the size
it is harder for complier to optimize that
#+end_notes
- **Key idea:** Unroll loops to manually do more things in one loop
#+begin_src c++
void extract_set_bits_optimized(uint64_t bitset, uint32_t* output) {
    // Get total number of set bits
    uint32_t count = __builtin_popcountll(bitset);
    uint32_t* next_base = output + count;

    // Process 8 bits at a time unconditionally
    while (bitset) {
        // Extract next 8 set bit positions, even if we don't have 8 bits
        *output++ = __builtin_ctzll(bitset);
        bitset &= (bitset - 1);  // Clear lowest set bit (blsr instruction)

        *output++ = __builtin_ctzll(bitset);
        bitset &= (bitset - 1);
#+end_src
** Compiler Directives & Special Build Flags
- Compiler flags (for instance, -O3 or -march=native) and specific macros are key to unlocking peak performance.
** C++ optimizations summary
- Zero cost abstractions
- inline functions and casting
- Error code over exceptions
- memory and loop optimizations
* Thank you
