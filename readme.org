#+TITLE: Parsing Gigabytes of JSON per Second
#+AUTHOR: Wong Ding Feng
#+LANGUAGE: en
#+OPTIONS: num_lines:t toc:2 ^:nil
#+REVEAL_THEME: moon
#+EXPORT_SELECT_STRINGS: ((org-export-string "latex") "\\usepackage{amsmath} \\usepackage{amsthm} \\usepackage{amssymb}")
* Objectives
#+begin_notes
Too many things to talk about
pick and choose some
#+end_notes
- Describe the real problem with JSON
  - Techniques and strategies to make it fast
- Simple primer on bitwise operations and simd
- simdjson architecture
- How this is used to make simdjson
* Problem
** How fast is your hardware
#+begin_notes
One core on your computer can actually read and write around 3GB of data per second
#+end_notes

CPU: AMD Eng Sample: 100-000000534-40_Y: 4.6 GHz
Network: 1Gb~100MB/s, 10Gb~1000MB/s

One core can read direct text at this speed:
#+begin_src text
Testing dd read:
1280+0 records in
1280+0 records out
10737418240 bytes (11 GB, 10 GiB) copied, 3.88076 s, 2.8 GB/s
#+end_src
** Why is json interesting?
- most data is in json
- basically double the speed means you can parse 2x as much data
  - duplicating hardware for free
| parser    | Skylake | Cannon Lake | speed  |
|-----------+---------+-------------+--------|
| simdjson  |     1.4 |         1.3 | fast   |
| RapidJSON |    0.56 |        0.44 | slow   |
| sajson    |    0.93 |        0.84 | normal |
** Why you should be interested
- configurable, increase speed
#+ATTR_HTML: :style background-color: white;
[[https://arxiv.org/html/1902.08318v7/x1.png]]
** Against others
#+ATTR_HTML: :style background-color: white;
[[https://arxiv.org/html/1902.08318v7/x3.png]]
** On demand json
#+ATTR_HTML: :style background-color: white;
[[https://arxiv.org/html/2312.17149v3/x1.png]]
* Ideas on how to do it fast?
#+begin_notes
Suppose we think we have already written the fastest possible cpu parser in the world, what other ways can we speed it up?
#+end_notes
** strategies
- depends on the usage pattern
*** Query intensive
- Create a database (ElasticSearch, MongoDB, PostgresSQL)
  - create a KV store
  - load once and query it
*** Selective parsing
#+begin_notes
The other opposite end is no parsing,
Skip the parsing as parsing takes time, NoDB, do a grep search and jump around detecting some structures and patterns in the data
There was a research paper talking about using JIT and speeding up the json query like a compiler
Mison is another implementation that uses simd to find important character locations like braces [] " : and the authors of simdjson learnt lots from them.
#+end_notes
- Selective parsing
  - NoDB
    - query the data without parsing it, without loading into a DB
    - like grep
  - JIT techniques
    - find patterns and repetitive structures, compile the code for the specific query
    - like a compiler
  - *Mison* (by Microsoft)
    - selective parsing, jump directly to the field you want
    - use SIMD to find structural important characters like "
** What is fair game?
#+begin_notes
So there are many json implementations out there and to measure performance, we need to properly define what json means.

Most faster json parser implementations play cheat by just assuming the input is already valid.

assuming strings are only ascii when json RFC said UTF-8, dont validate numbers, selectively parsing.

simdjson is a complete parser following Json RFC standards, fully validating the input yet being faster than all of them. assuming input is correct is dangerous because it just is wrong input and wrong output
#+end_notes
- Types of json parsing
  - Non-validating json parser
    - assume the input is valid
    - easier
    - most selective parsing is non-validating
  - Validating json parser
    - check the input is valid
    - no assumptions or malformed input
      - security risk
      - its just wrong number or string being parsed
    - harder more complex
** Proper definition of JSON
#+begin_notes
This is the real EBNF grammar for json, its kinda complex so i wrote a simplified version below
#+end_notes

#+begin_src ebnf
/* JSON EBNF Grammar Specification */

/* Root JSON structure */
json = ws , (object | array) , ws ;

/* Objects */
object = "{" , ws , [ members ] , ws , "}" ;
members = pair , { "," , ws , pair } ;
pair = string , ws , ":" , ws , value ;

/* Arrays */
array = "[" , ws , [ elements ] , ws , "]" ;
elements = value , { "," , ws , value } ;

/* Values */
value = string | number | object | array | "true" | "false" | "null" ;

/* Strings */
string = '"' , { char | escape } , '"' ;
char = ? any Unicode character except " or \ or control characters ? ;
escape = "\" , ('"' | "\" | "/" | "b" | "f" | "n" | "r" | "t" | unicode) ;
unicode = "u" , hexdigit , hexdigit , hexdigit , hexdigit ;
hexdigit = digit | "A" | "B" | "C" | "D" | "E" | "F" | "a" | "b" | "c" | "d" | "e" | "f" ;

/* Numbers */
number = [ "-" ] , (zero | integer) , [ fraction ] , [ exponent ] ;
integer = nonzero , { digit } ;
nonzero = "1" | "2" | "3" | "4" | "5" | "6" | "7" | "8" | "9" ;
digit = "0" | nonzero ;
zero = "0" ;
fraction = "." , digit , { digit } ;
exponent = ("E" | "e") , [ "+" | "-" ] , digit , { digit } ;

/* Whitespace */
ws = { whitespace } ;
whitespace = " " | "\t" | "\n" | "\r" ;

/* Comments and Explanation */
#+end_src

** My reduced definition of JSON
#+begin_notes
A json value is just made up of 2 value types
primitive value and container value

primitive is just like all the java primitives that we know of, boolean string number null

container types only has 2 types array and objects

arrays can contain any json value
objects contain {string: json_value}
#+end_notes

#+begin_src ebnf
json_value ::= primitive_value | container_value

primitive_value ::= boolean | string | number | null

boolean ::= "true" | "false"

string ::= " utf8_char* "
utf8_char ::= ascii_char | unicode_char

number ::= integer | decimal | scientific
integer ::= ["+" | "-"] digit+
decimal ::= integer "." digit+
scientific ::= decimal "e" ["+"|"-"] digit+
digit ::= 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9

null ::= "null"

container_value ::= object | array

object ::= "{" string:json_value, string:json_value "}"

array ::= "[" json_value, json_value "]"

(* Notes:
   - Integer limits: [-2^53+1 to 2^53-1] for safe integers
   - Scientific notation limit: approximately ±2^1024
   - NaN and Infinity are not valid JSON numbers
   - Strings must be UTF-8 encoded
,*)
#+end_src
** Strongly define: bool, string, number, null, object and array
#+begin_notes
very simple, we just need to strongly define these 6 basic types
bool, string, number, null, object and array
write parsing rules to validate and check that they are valid
then understand what the data means

This sounds simple, but it is deceptively simple
#+end_notes
#+begin_src haskell
data JsonValue
    = Primitive PrimitiveValue
    | Container ContainerValue

-- 6 primitives --------------------------
data PrimitiveValue
    = Boolean Bool
    | String Text
    | Number Double
    | Null

data ContainerValue
    = Object Object
    | Array Array
-- END -----------------------------------

newtype Object = Object [(Text, JsonValue)]

newtype Array = Array [JsonValue]
#+end_src
** Strongly define: bool, string, number, null, object and array
#+begin_notes
boolean, true, false and null are trivial

So first we start with the simplest sounding one of all, number, just integers right?, decimal perhaps? easy!
#+end_notes
*** Number limits and Integers
#+begin_notes
Lets take a look at limits.
Theres no strict definition for the limit of numbers, most use (2^53 - 1) because of the floating point representation
the authors of SIMDjson prefer 2^63 - 1
the first special case we have to deal with is negative numbers, we cant only detect 0 - 9, we have to detect - as well.
#+end_notes
#+begin_src javascript
// 1. Integer Limits
const INTEGER_EXAMPLES = {
    // Maximum safe integer in JavaScript (2^53 - 1)
    max_safe_integer: 9007199254740991,
    // Minimum safe integer in JavaScript (-(2^53 - 1))
    min_safe_integer: -9007199254740991,

    // Zero representations
    zero: 0,
    negative_zero: -0,  // JSON preserves negative zero

    // Common boundary values
    max_32bit_int: 2147483647,
    min_32bit_int: -2147483648,

    // Integer examples
    positive: 42,
    negative: -42
};
#+end_src
*** Floats and Scientific notation
#+begin_notes
Floats, you see the e-308.
below you can see that both E and e are valid
some + and some - exponents
some without the + and - signs

what about the special case of 0.0e0!? how do we handle that?
these are all the details your validator needs to check for before declaring that this is a valid input
#+end_notes
#+begin_src javascript
// 2. Floating Point Examples
const FLOAT_EXAMPLES = {
    // Precision examples (up to 15-17 significant digits)
    high_precision: 1.234567890123456,

    // Edge cases
    very_small_positive: 2.2250738585072014e-308, // Near smallest possible double
    very_large_positive: 1.7976931348623157e+308  // Near largest possible double
};

// 3. Scientific Notation Examples
const SCIENTIFIC_NOTATION = {
    // Positive exponents
    large_scientific: 1.23e+11,
    very_large: 1.23E+308,  // Note: Both 'e' and 'E' are valid

    // Negative exponents
    small_scientific: 1.23e-11,
    very_small: 1.23E-308,

    // Zero with exponent
    zero_scientific: 0.0e0,

    // Various representations
    alternative_forms: {
        standard: 1230000000,
        scientific: 1.23e9,
        another_form: 123e7
    }
};


#+end_src
** String: handle escaped quotes and UTF-8
#+begin_notes
next we have string, many implementations just assume ascii

json RFC says it must be UTF-8

the last important thing to take note of is escaped \", we need to detect them properly to get the correct json, everything is done in simd.
#+end_notes
- some lazy parsers assume ascii for simplicity
  - 128 possibilities, 8 bits only
  - assume that input does not have japanese or chinese or weird characters
- RFC standard says strings are UTF-8
- escaped double quotes "Tom said: \"hello\"."
  - Tom said: "hello".
  - number of '\'
    - odd -> escaped, "\"" -> "
    - even -> not escaped, "\\" -> \
- outside of ",there can only be 4 types of white space
  - " " | "\t" | "\r" | "\n"
*** ASCII code
#+begin_notes
This is just simply the ascii code table, quite sure we are all very familiar with it 0x30 - 0x39 is digits 0-9 lets move on
#+end_notes
- code ponits 0x00 - 0xEF 127 possibilities
#+ATTR_HTML: :style background-color: white;
[[https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/USASCII_code_chart.svg/1280px-USASCII_code_chart.svg.png]]
*** UTF-8
#+begin_notes
if it starts with the first bit being 0, it is ASCII
else if it is 1, it must conform to UTF-8 standards
#+end_notes
#+begin_src text
Single byte (ASCII):
0xxxxxxx                     (values 0-127)
Values start with 0, remaining 7 bits for data

Two bytes:
110xxxxx 10xxxxxx           (values 128-2047)
First byte starts with 110

Three bytes:
1110xxxx 10xxxxxx 10xxxxxx  (values 2048-65535)
First byte starts with 1110

Four bytes:
11110xxx 10xxxxxx 10xxxxxx 10xxxxxx   (values 65536+)
First byte starts with 11110
#+end_src
- normally outside of strings, no bytes start with 1 in front.
** Summary of requirements
- numbers
  - negative
  - floats
- string
  - utf-8
  - escaped quotes \"
- Rest of structure well formed
  - valid whitespace
  - valid bracket {}, []
* challenges
** writing a parser for it
#+begin_notes
Im not sure how many of us here has written a recursive descent parser but normally
how one would write a parser is that
one would just scan the string from left to right until it can determine what to do with the input
this requires many if else checks to see when to stop, when to look back, when to decide that what I am seeing is an object, string, array ...

the problem is that if statements cause a miss predicted branch, this is very costly to computers
if the branching is very predictable, like taking true all the time, there is no cost, usually the cpu will just
assume the previous branch was taken and follow that, then the cost is 1 cycle

if we need to stop and recorrect the branch it will take at least 15 cycles.

Can you do it without branches? thats what the SIMDjson team was working on.
#+end_notes
- Recursive Descent type parser
- Many if else required, is it possible to do it without any branches?
#+begin_src python
def peek_token_type(json_str, index):
    char = json_str[index]

    # Skip whitespace
    while index < len(json_str) and is_whitespace(char):
        index += 1
        char = json_str[index]

    # Check data type based on first character
    if char == '{':
        return 'object'
    elif char == '[':
        return 'array'
    elif char == '"':
        return 'string'
    elif is_digit(char):
        return 'number'
    elif char == 't' or char == 'f':
        return 'boolean'
    elif char == 'n':
        return 'null'
    else:
        raise ValueError(f"Invalid JSON character at position {index}: {char}")
#+end_src
** Given the challenge, how to do it fast?
#+begin_notes
mison already implemented some of these but not everything
#+end_notes
- Parallelization, split work across multiple cores.
- SIMD, process more than 8 bytes at a time.
  - Branchless code, no if statements. CPU missed branch prediction.
    - correct, 0-1 cycles
    - branch miss, 20 cycles
* About SIMD
how does simd fit into all of this?
** What is simd
[[https://pep-root6.github.io/docs/analysis/simd.png]]
** SIMD example
#+BEGIN_EXAMPLE
Adding 4 numbers simultaneously:

Scalar:
A: [5] + [3] = [8]     Step 1
B: [7] + [2] = [9]     Step 2
C: [4] + [6] = [10]    Step 3
D: [1] + [8] = [9]     Step 4

SIMD:
[5|7|4|1] +
[3|2|6|8] =   Step 1
[8|9|10|9]    Done!
#+END_EXAMPLE
** CPU
#+begin_src text
Year:         2010          2013          2019
Architecture: Westmere  ->  Haswell   ->  Ice Lake
Process:      32nm          22nm          10nm
Vector ISA:   SSE2      ->  AVX2      ->  AVX512
Vec Width:    128-bit       256-bit       512-bit
             (16 bytes)    (32 bytes)    (64 bytes)
#+end_src
- Streaming SIMD Extensions
  - XMM0-XMM15
- Advanced Vector Extensions 2
  - YMM0-YMM15
- Advanced Vector Extensions 512
  - ZMM0-ZMM15
** SIMD code is not that scary
Westmere uses 128-bit SSE instructions     (_mm_shuffle_epi8)
Haswell  uses 256-bit AVX2 instructions    (_mm256_shuffle_epi8)
Ice Lake uses 512-bit AVX-512 instructions (_mm512_shuffle_epi8)
#+begin_src cpp
// Westmere
const uint64_t whitespace = in.eq({
    _mm_shuffle_epi8(whitespace_table, in.chunks[0]),
    _mm_shuffle_epi8(whitespace_table, in.chunks[1]),
    _mm_shuffle_epi8(whitespace_table, in.chunks[2]),
    _mm_shuffle_epi8(whitespace_table, in.chunks[3])
});

// Haswell (2 x 256-bit chunks)
const uint64_t whitespace = in.eq({
    _mm256_shuffle_epi8(whitespace_table, in.chunks[0]),
    _mm256_shuffle_epi8(whitespace_table, in.chunks[1])
});

// Ice Lake (1 x 512-bit chunk)
const uint64_t whitespace = in.eq({
    _mm512_shuffle_epi8(whitespace_table, in.chunks[0])
});
#+end_src
** When SIMD Shines
- Regular, predictable data patterns
- Simple mathematical operations
- Continuous blocks of memory
- Identical operations on multiple data points
- High throughput
#+BEGIN_EXAMPLE
Perfect for SIMD:
[1|2|3|4] × 2  = [2 |4 |6 |8 ] ✓
[R|G|B|A] + 10 = [R'|G'|B'|A'] ✓
#+END_EXAMPLE
** SIMD's Achilles Heel: Branching
- if logic is complex like in parsing unable to do simd
#+begin_src c++
    if (char_at == '{') {
        return "object";
    } else if (char_at == '[') {
        return "array";
    } else if (char_at == '"') {
        return "string";
    } else if (is_digit(char_at)) {
        return "number";
    } else if (char_at == 't' || char_at == 'f') {
        return "boolean";
    } else if (char_at == 'n') {
        return "null";
    } else {
        throw std::invalid_argument(
            "Invalid JSON character at position " +
            std::to_string(index) +
            ": " + char_at
        );
    }
#+end_src
*** Arithmetic booleans
- actually LLVM does this for you when you do -o2 and -o3
#+begin_src c++
    // Example 1: Arithmetic with booleans
    bool condition = true;
    int a = 10;
    int b = 20;

    // Branched version
    int x;
    if (condition) {
        x = a;
    } else {
        x = b;
    }
    std::cout << x << std::endl;  // Output: 10

    // Branchless version 1
    x = condition * a + (!condition) * b;
    // Step by step:
    // true * 10 + (!true) * 20
    // 1 * 10 + 0 * 20
    // 10 + 0 = 10
    std::cout << x << std::endl;  // Output: 10

    // Branchless version 2
    x = b + (a - b) * condition;
    // Step by step:
    // 20 + (10 - 20) * true
    // 20 + (-10) * 1
    // 20 - 10 = 10
    std::cout << x << std::endl;  // Output: 10
#+end_src
*** Selection indexing
- actually LLVM does this for you when you do -o2 and -o3
#+begin_src c++
    // Example 2: Tuple indexing
    bool condition = true;
    int a = 10;
    int b = 20;

    // Branched version
    int x;
    if (condition) {
        x = a;
    } else {
        x = b;
    }
    std::cout << x << std::endl;  // Output: 10

    // Branchless version
    std::array<int, 2> values = {b, a};  // Note: array order is {b, a} to match Python's (b, a)
    x = values[condition];
    // Step by step:
    // {20, 10}[true]
    // {20, 10}[1]     // true converts to 1
    // 10
    std::cout << x << std::endl;  // Output: 10

    return 0;
#+end_src
*** If LLVM does it for you, whats the point?
#+begin_notes
LLVM only good at small cases.
For larger complex patterns like JSON.
The authors noticed several patterns in the data.
Exploited them and made all operations into SIMD.
Also by batching operations together like maybe do 1 type of operation over the entire string
We can basically almost use SIMD for the entire parsing instead of small minor optimizations.
#+end_notes
- LLVM does it's best, but it cannot find everything
  - good at small cases
- some larger complex patterns
  - human pattern recognition
  - batching operations you can use simd
** Write branchless code (bitwise operations)
*** Tricky memory layout
#+begin_src text
number = 305,419,896
number << 1 # shift left logical
Number: 305,419,896
Hex: 0x12345678
Physical Memory Layout (lowest bit → highest bit)
   Addr Low                           Addr High
     0x1200                              0x1203
        |                                 |
        v                                 v
Before: 00011110 01101010 00110100 00010010
           ↓↓↓↓↓    ↓↓↓↓↓    ↓↓↓↓↓    ↓↓↓↓↓
After:  00001111 00110101 00010110 00100100
        ↑
        0 enters
Decimal: 610,839,792
Hexadecimal: 0x2468ACF0
#+end_src
#+begin_notes
need to know some low level operations to explain all the SIMD things later
#+end_notes
*** Masking
#+begin_src
a = 00001111
b = 11111100

and_op = a & b
and_op = 00001100

 or_op = a | b
 or_op = 11111111

xor_or = a ^ b
xor_or = 11110011
#+end_src
*** Unset right most bit(blsr)
#+begin_src
s = s & (s-1)

a =     00101100
b =     (a - 1)
a =     00101100
b =     00101011
a & b = 00101000
// rightmost bit is unset
#+end_src
- common cpu operation, compiler optimize to ~blsr~
** LLVM Compiler
#+begin_notes
One of the things to do to write fast code is know how the LLVM compiler optimizes your code
There are many many optimizations available for us to use, so I will not go through them,
I'll just talk abit about how optimizations even work in the first place.
#+end_notes
[[https://llvm.org/img/LLVMWyvernSmall.png]]
*** LLVM
#+begin_notes
LLVM is split up into 3 parts,
Frontend, middle-end and backend

the front end will read our c++ source code and output something called a Intermediate Representation

Then the IR will be optimized then the backend will target different cpu platforms like X86, ARM and PowerPC

Thats all LLVM is, its not that difficult
#+end_notes
#+begin_src text
      Frontend         Middle-end              Backend
             ↓                ↓              ↙         ↘
Source Code → LLVM IR → [Optimized IR] → [Assembly] → Machine Code
                                     ↘________________↗
                                     (direct path option)
#+end_src
*** Without LLVM IR
#+begin_notes
Why do we need to have this IR
Every new language we add we need to write compilers to target all the outputs
#+end_notes
#+begin_src text
Without LLVM IR (n*m: 3 languages × 3 targets = 9 compilers)
---------------------------------------------------------
C++   ----→  x86_64
      \---→  AMD
       \--→  ARM

Rust  ----→  x86_64
      \---→  AMD
       \--→  ARM

Haskell --→  x86_64
        \-→  AMD
         \→  ARM

Each arrow represents a separate compiler frontend+backend (9 total)
#+end_src
*** With LLVM IR
#+begin_notes
We just write one compiler to target the IR then it can just generate the output for each architecture
Any optimizations and improvements to the compiler on the right side, we get it for free on the left side.
#+end_notes
#+begin_src text
With LLVM IR (n+m: 3 frontends + 3 backends = 6 components)
--------------------------------------------------------

            ╭→ x86_64
C++    ╮    │
       ↓    │
Rust   ━━→ IR ━━→ AMD
       ↑    │
Haskell╯    │
            ╰→ ARM

            ┊
            ↓
    Shared Optimizations
    - Dead code elimination
    - Loop vectorization
    - Constant propagation
    - And many more...
#+end_src
*** Intermediate Representation Example(IR)
#+begin_notes
Lets talk about an example code here very simple for loop
sum += i*4
return
#+end_notes
#+begin_src c++
int example2(int n) {
    int sum = 0;
    for (int i = 0; i < n; i++) {
        sum += i * 4;  // Multiplication in loop
    }
    return sum;
}
#+end_src
*** Unoptimized IR -O0
#+begin_notes
If we compile with -O0, optimization level 0, no optimization
This is the IR
its this cpu agnostic code that has as much information retained from the original source code as possible
this is because to optimize, the compiler has to know what data types its dealing with
and then it can draw a computation graph to eliminate useless computation
#+end_notes
#+begin_src llvm-ts
define dso_local i32 @_Z8example2i(i32 %0) {
entry:
  %n = alloca i32, align 4
  %sum = alloca i32, align 4
  %i = alloca i32, align 4
  store i32 %0, ptr %n, align 4
  store i32 0, ptr %sum, align 4
  store i32 0, ptr %i, align 4
  br label %for.cond

for.cond:
  %1 = load i32, ptr %i, align 4
  %2 = load i32, ptr %n, align 4
  %cmp = icmp slt i32 %1, %2
  br i1 %cmp, label %for.body, label %for.end

for.body:
  %3 = load i32, ptr %i, align 4
  %mul = mul nsw i32 %3, 4
  %4 = load i32, ptr %sum, align 4
  %add = add nsw i32 %4, %mul
  store i32 %add, ptr %sum, align 4
  br label %for.inc

for.inc:
  %5 = load i32, ptr %i, align 4
  %inc = add nsw i32 %5, 1
  store i32 %inc, ptr %i, align 4
  br label %for.cond

for.end:
  %6 = load i32, ptr %sum, align 4
  ret i32 %6
}
#+end_src
*** Unoptimized IR -O0 Graph
#+begin_notes
this is the computation graph of the IR in front
#+end_notes
#+begin_src mermaid :file attachments/unoptimized-ir.png
flowchart TD
    classDef memop fill:#f9f,stroke:#333
    classDef arithop fill:#afd,stroke:#333
    classDef control fill:#fda,stroke:#333

    param["%0 param"]

    subgraph entry
        alloc_n["%n = alloca"]:::memop
        alloc_sum["%sum = alloca"]:::memop
        alloc_i["%i = alloca"]:::memop
        store_n["store %0 to %n"]:::memop
        store_sum0["store 0 to %sum"]:::memop
        store_i0["store 0 to %i"]:::memop
    end

    subgraph for_cond
        load_i1["load from %i"]:::memop
        load_n["load from %n"]:::memop
        cmp["icmp slt"]:::arithop
        branch_cond["br i1"]:::control
    end

    subgraph for_body
        load_i2["load from %i"]:::memop
        mul["mul * 4"]:::arithop
        load_sum["load from %sum"]:::memop
        add["add"]:::arithop
        store_sum["store to %sum"]:::memop
    end

    subgraph for_inc
        load_i3["load from %i"]:::memop
        inc["add + 1"]:::arithop
        store_i["store to %i"]:::memop
    end

    subgraph for_end
        load_sum_final["load from %sum"]:::memop
        ret["return"]:::control
    end

    param --> store_n
    alloc_n --> store_n
    alloc_sum --> store_sum0
    alloc_i --> store_i0

    store_i0 --> load_i1
    store_n --> load_n
    load_i1 --> cmp
    load_n --> cmp
    cmp --> branch_cond
    branch_cond -->|"i < n"| load_i2
    branch_cond -->|"i >= n"| load_sum_final

    load_i2 --> mul
    mul --> add
    load_sum --> add
    add --> store_sum
    store_sum --> load_i3

    load_i3 --> inc
    inc --> store_i
    store_i --> load_i1

    load_sum_final --> ret
#+end_src

#+RESULTS:
[[file:attachments/unoptimized-ir.png]]

*** Optimized IR -O2
#+begin_notes
this is compiled with -O2
#+end_notes
#+begin_src llvm-ts
define dso_local i32 @_Z8example2i(i32 %0) local_unnamed_addr #0 {
entry:
  %cmp6 = icmp sgt i32 %0, 0
  br i1 %cmp6, label %for.body.preheader, label %for.end

for.body.preheader:
  %1 = add i32 %0, -1
  %2 = mul i32 %0, %1
  %3 = lshr i32 %2, 1
  %4 = mul i32 %3, 4
  br label %for.end

for.end:
  %sum.0.lcssa = phi i32 [ 0, %entry ], [ %4, %for.body.preheader ]
  ret i32 %sum.0.lcssa
}
#+end_src

*** Optimized IR -O2 Graph
#+begin_notes
with deadcode eliminated
#+end_notes
#+begin_src mermaid :file attachments/optimized-ir.png
flowchart TD
    classDef arithop fill:#afd,stroke:#333
    classDef control fill:#fda,stroke:#333

    param["%0 param"]

    subgraph entry
        cmp["icmp sgt i32 %0, 0"]:::arithop
        branch["br i1"]:::control
    end

    subgraph for_body_preheader
        sub["add i32 %0, -1"]:::arithop
        mul1["mul i32 %0, %1"]:::arithop
        shift["lshr i32 %2, 1"]:::arithop
        mul2["mul i32 %3, 4"]:::arithop
    end

    subgraph for_end
        phi["phi i32 [0, entry], [%4, preheader]"]:::control
        ret["ret i32"]:::control
    end

    param --> cmp
    cmp --> branch
    branch -->|"> 0"| sub
    branch -->|"<= 0"| phi

    sub --> mul1
    param --> mul1
    mul1 --> shift
    shift --> mul2
    mul2 --> phi

    phi --> ret

    style param fill:#ddd
    style ret fill:#f96
#+end_src

#+RESULTS:
[[file:attachments/optimized-ir.png]]

* Simdjson Implementation
** simdjson Architecture Overview
1. Stage 1: Structural Index Creation (find location of important markers)
   1. Find structural characters ({,},[,],",,:)
   2. Identify string boundaries
   3. Locate whitespace
   4. Validate UTF-8 encoding
   5. Detect pseudo-structural characters
2. Stage 2: Parsing & Tape Building
   1. Parse atomic values (strings, numbers, true/false/null)
   2. Validate document structure
   3. Build navigable tape representation
   4. Convert numbers to machine formats
   5. Normalize strings to UTF-8
** simdjson diagram
#+begin_src text
    JSON INPUT STRING
   "{"name": "value"}"
            ⬇
     64-BYTE CHUNKS
   ╔═════════════════╗
   ║"{"name": "val...║
   ╚═════════════════╝
            ⬇
         STAGE 1
  (Bitmap Generation &   find: ([, {, ], }, :, ,)
   Index Extraction)     escaped characters and quoted regions
            ⬇            Validate UTF-8
       INDEX ARRAY
      [0,3,5,7,...]
            ⬇
         STAGE 2         parse number, int, float, 1e10, true, false, null, string
   (Parse & Build Tape)  build tape to navigate
            ⬇
       FINAL TAPE
[root, {, "name", "value", }]
#+end_src
** Stage 1: Structural and Pseudo Structural Index Construction
*** Input and Output
- Input: Raw JSON bytes
- Output:
  - Bitmask of structural chars
  - Array of integer indices marking structural elements

*** Key Responsibilities
1. Character encoding validation (UTF-8)
2. Locate structural characters ([, {, ], }, :, ,)
3. Identify string boundaries
   1. Handles escaped characters and quoted regions
4. Find pseudo-structural characters (atoms like numbers, true, false, null)
** Stage 2: Structured Navigation
*** Input and Output
- Input: Array of structural indices from Stage 1
- Output: Parsed JSON structure on a "tape"(array)
- Purpose: Build navigable representation of JSON document

*** Key Responsibilities
1. Parse strings and convert to UTF-8
2. Convert numbers to 64-bit integers or doubles
3. Validate structural rules (matching braces, proper sequences)
4. Build navigable tape structure

*** The Tape Format
- 64-bit words for each node
- Special encoding for different types:
  - Atoms (null, true, false): n/t/f × 2^56
  - Numbers: Two 64-bit words
  - Arrays/Objects: Start/end markers with navigation pointers
  - Strings: Pointer to string buffer

* Stage 1: Structural and Pseudo Structural Index Construction
#+begin_src cpp
  const auto whitespace_table = simd8<uint8_t>::repeat_16(' ', 100, 100, 100, 17, 100, 113, 2, 100, '\t', '\n', 112, 100, '\r', 100, 100);

  const auto op_table = simd8<uint8_t>::repeat_16(
    0, 0, 0, 0,
    0, 0, 0, 0,
    0, 0, ':', '{', // : = 3A, [ = 5B, { = 7B
    ',', '}', 0, 0  // , = 2C, ] = 5D, } = 7D
  );

  const uint64_t whitespace = in.eq({
    _mm256_shuffle_epi8(whitespace_table, in.chunks[0]),
    _mm256_shuffle_epi8(whitespace_table, in.chunks[1])
  });
  // Turn [ and ] into { and }
  const simd8x64<uint8_t> curlified{
    in.chunks[0] | 0x20,
    in.chunks[1] | 0x20
  };
  const uint64_t op = curlified.eq({
    _mm256_shuffle_epi8(op_table, in.chunks[0]),
    _mm256_shuffle_epi8(op_table, in.chunks[1])
  });

  return { whitespace, op };
#+end_src
** Stage 1: 1 Vectorized Classification and Pseudo-Structural Characters
- Want to obtain location of structural characters  ({, }, [, ], :, ,)
  - pseudo-structural - Any non‐whitespace character that immediately follows a structural character or whitespace
  - useful for parsing, we need this bit mask to build tape
#+begin_src text
{ "\\\"Nam[{": [ 116,"\\\\" , 234, "true", false ], "t":"\\\"" }
__1______________1___1________1____1_______1________1___1_______
#+end_src
*** Vectorized Classification
#+begin_notes
We need to classify structural characters
each different class gets its own type
we need to do this classification fast we will use a look up table to do the classification, basically O(1)
#+end_notes
| code points | character   | desired value |   bin |
|-------------+-------------+---------------+-------|
|        0x2c | `,` (comma) |             1 | 00001 |
|        0x3a | `:` (colon) |             2 | 00010 |
|        0x5b | `[`         |             4 | 00100 |
|        0x5d | `]`         |             4 | 00100 |
|        0x7b | `{`         |             4 | 00100 |
|        0x7d | `}`         |             4 | 00100 |
|        0x09 | TAB         |             8 | 01000 |
|        0x0a | LF          |             8 | 01000 |
|        0x0d | CR          |             8 | 01000 |
|        0x20 | SPACE       |            16 | 10000 |
|      others | any other   |             0 | 00000 |
- PMOVMSKB
  - _mm256_movemask_epi8 to extract the bits into bitmap
**** VPSHUFB: Vector Permute Shuffle Bytes
#+begin_notes
If you have any experience with hashmaps, they are actually very slow
they are not truely o(1) lookup
the only true O(1) lookup structures are actually arrays, index + offset
hashing function is a fake O(1)
#+end_notes
- basically a one instruction lookup table using the 4 lowest bit(nibble)
  - 0000 XXXX
#+begin_src c++
int main() {
    // Lookup table for hex digits "0123456789abcdef"
    __m256i lut = _mm256_setr_epi8(
        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f',
        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f'
    );

    // Example 2: Alternating normal/zeroed values (0x00,0x80,0x01,0x81...)
    __m256i indices2 = _mm256_setr_epi8(
        0x00, 0x80, 0x01, 0x81, 0x02, 0x82, 0x03, 0x83, 0x04, 0x84, 0x05, 0x85, 0x06, 0x86, 0x07, 0x87,
        0x08, 0x88, 0x09, 0x89, 0x0A, 0x8A, 0x0B, 0x8B, 0x0C, 0x8C, 0x0D, 0x8D, 0x0E, 0x8E, 0x0F, 0x8F
    );

    printf("\nAlternating with zeroes (. represents zero):\n");
    print_bytes(_mm256_shuffle_epi8(lut, indices2));
    // Alternating with zeroes (. represents zero):
    // 0.1.2.3.4.5.6.7.8.9.a.b.c.d.e.f.

    return 0;
}

#pragma GCC target("avx2")
#include <immintrin.h>
#include <stdio.h>
void print_bytes(__m256i v) {
    unsigned char bytes[32];
    _mm256_storeu_si256((__m256i*)bytes, v);
    for(int i = 0; i < 32; i++) {
        if (bytes[i]) {
            printf("%c", bytes[i]);
        } else {
            printf(".");  // Print dot for zero bytes
        }
    }
    printf("\n");
}
#+end_src
**** Simple example
| code points | character   | desired value |   bin |
|        0x3a | `:` (colon) |             2 | 00010 |
|        0x0a | LF          |             8 | 01000 |
- use vpshufb to match low nibble a
- could be both : and LF so it must match 0010 | 1000 = 1010
- low nibble at position A = 10
  - high nibble 0x3 vs 0x0
    - 0x3 = 2
    - 0x0 = 8
***** Simple example
#+begin_example
"LF:"

Low nibble table
00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15
xx xx xx xx xx xx xx xx xx xx 10 xx xx xx xx xx
1010

high nibble table
00 .. 02 03 04 05 06 07 08 09 10 11 12 13 14 15
08 .. 02 xx xx xx xx xx xx xx xx xx xx xx xx xx
0100,  0010
#+end_example
***** Simple example
|     |      |   LF |    : |
|     | low  | 1010 | 1010 |
|     | high | 1000 | 0010 |
| AND |      | 1000 | 0010 |
|     |      |    8 |    2 |

*** Stage 1: Bitmap to Array index
**** input data
#+begin_notes
In stage 1, we our functions take in 64 byte * 8 bit blocks
and output bitmasks of 64 bits each
we have several bit mask types

quotes, between quotes, structure, whitespace

however these masks are sparse, sometimes it can be 4 char before we a faced with a 1
sometimes the spaces are 4, and the spaces could be 40.

if we iterate through this and process it with if else statements, its unpredictable branching and will cause performance penalty, mson does this

as such we want to extract the bits into a list of indexes instead of working directly with the bitsets.
#+end_notes

#+begin_quote
{ "\\\"Nam[{": [ 116,"\\\\" , 234, "true", false ], "t":"\\\"" }: input data
__1_________1________1____1________1____1___________1_1_1____1__: Q
1_________11_1_1____1_______1____1_______1_______11____1_______1: S
_1____________1_1__________1_1____1_______1_____1__1__________1_: W
#+end_quote
- take Q for example, we want to convert Q's bit mask into a list of indexes
  - [2, 12, 22, 27, 37, 42, 54, 56, 58, 62]
**** extraction
- 2 instructions
- tzcnt count trailing least significant 0 bits
- blsr which delete the last bit.
#+begin_src text
a = 1010000
idx = tzcnt(a) // 4
a = blsr(a)    // 1000000
idx = tzcnt(a) // 6
#+end_src
**** Naive Implementation
#+begin_notes
the compiler will automatically optimize this into the tzcnt and blsr
that while loop is the part with the unpredictable branching which will cost 10-20 cycles for every wrong prediction
how do we solve it?
#+end_notes
#+begin_src c++
void extract_set_bits_unoptimized(uint64_t bitset, uint32_t* output) {
    uint32_t pos = 0;

    // This while loop is the source of unpredictable branches
    while (bitset) {
        // Find position of lowest set bit
        uint32_t bit_pos = __builtin_ctzll(bitset);
        // Store the position
        *output++ = bit_pos;
        // Clear the lowest set bit
        bitset &= (bitset - 1);
    }
}
#+end_src
**** Minimal branching implementation
#+begin_src c++
void extract_set_bits_optimized(uint64_t bitset, uint32_t* output) {
    // Get total number of set bits
    uint32_t count = __builtin_popcountll(bitset);
    uint32_t* next_base = output + count;

    // Process 8 bits at a time unconditionally
    while (bitset) {
        // Extract next 8 set bit positions, even if we don't have 8 bits
        *output++ = __builtin_ctzll(bitset);
        bitset &= (bitset - 1);  // Clear lowest set bit (blsr instruction)

        *output++ = __builtin_ctzll(bitset);
        bitset &= (bitset - 1);

        *output++ = __builtin_ctzll(bitset);
        bitset &= (bitset - 1);

        *output++ = __builtin_ctzll(bitset);
        bitset &= (bitset - 1);

        *output++ = __builtin_ctzll(bitset);
        bitset &= (bitset - 1);

        *output++ = __builtin_ctzll(bitset);
        bitset &= (bitset - 1);

        *output++ = __builtin_ctzll(bitset);
        bitset &= (bitset - 1);

        *output++ = __builtin_ctzll(bitset);
        bitset &= (bitset - 1);
    }

    // Reset output pointer to actual end based on real count
    output = next_base;
}
#+end_src
** Stage 1: 2 Eliminated escaped or quoted substring
*** Get backslash
#+begin_src text
{ "\\\"Nam[{": [ 116,"\\\\" , 234, "true", false ], "t":"\\\"" }: input data
___111________________1111_______________________________111____: B = backslash_bits
____111_________________1111______________________________111___: bits_shifted_left = backslash_bits << 1

___111________________1111_______________________________111____: bits
____000_________________0000______________________________000___: inverted = ~bits_shifted_left
___1__________________1__________________________________1______: S = starts = bits & inverted
// we get the first backslash of every group
#+end_src
*** Get odd length sequences starting on an odd offset
#+begin_src text
_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1: O (constant)
___111________________1111_______________________________111____: B = backslash_bits
___1__________________1__________________________________1______: S = starts = bits & inverted
_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1: O (constant)
___1_____________________________________________________1______: OS = S & O

// add B to OS, yielding carries on backslash sequences with odd starts
___1_____________________________________________________1______: OS = S & O
___111________________1111_______________________________111____: B = backslash_bits
   -->                                                   -->
______1_______________1111__________________________________1___: OC = B + OS

// filter out the backslashes from the previous addition, getting carries only
___111________________1111_______________________________111____: B = backslash_bits
___000________________0000_______________________________000____: ~B
______1_______________1111__________________________________1___: OC = B + OS
______1_____________________________________________________1___: OCO = OC & ~B

// get the odd-length sequence starting on an odd offset and ending on even offset
______1_____________________________________________________1___: OCO = OC & ~B
1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1: E (constant)
______1_____________________________________________________1___: OD2 = OCO & E
// this shows two odd-length sequence starting on an odd offset
#+end_src
*** Get odd length sequences starting on an even offset
its just the reverse of what we done just now
#+begin_src text
{ "\\\"Nam[{": [ 116,"\\\\" , 234, "true", false ], "t":"\\\"" }: input data
1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_: E (constant)
___1__________________1__________________________________1______: S = starts = bits & inverted
______________________1_________________________________________: ES = S & E
___111________________1111_______________________________111____: B = backslash_bits
// add B to ES, yielding carries on backslash sequences with even starts
                      --->
___111____________________1______________________________111____: EC = B + ES
// filter out the backslashes from the previous addition, getting carries only
__________________________1_____________________________________: ECE = EC & ~B
// select only the end of sequences ending on an odd offset
__________________________1_____________________________________: ECE = EC & ~B
_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1_1: O (constant)
________________________________________________________________: OD1 = ECE & ~E
// there are no odd-length sequences of backslashes starting on an even offset
#+end_src
*** Get sequences with odd offset
#+begin_src text
// merge results, yielding ends of all odd-length sequence of backslashes
________________________________________________________________: OD1 = ECE & ~E
______1_____________________________________________________1___: OD2 = OCO & E

______1_____________________________________________________1___: OD = OD1 | OD2
{ "\\\"Nam[{": [ 116,"\\\\" , 234, "true", false ], "t":"\\\"" }: input data

// these " are escaped and thus are counted as text instead of structural characters
#+end_src
*** Eliminated escape
#+begin_src text
{ "\\\"Nam[{": [ 116,"\\\\" , 234, "true", false ], "t":"\\\"" }: input data
__1___1_____1________1____1________1____1___________1_1_1___11__: Q = quotes
______1_____________________________________________________1___: OD
// we remove the escaped " quotes
__1_________1________1____1________1____1___________1_1_1____1__: Q &= ~OD
__1111111111_________11111_________11111____________11__11111___: CLMUL(Q,~0)
#+end_src
*** Sweeping
#+begin_src c++
uint64_t xorShiftOperations(uint64_t num, bool rightShift, bool tutorial = false) {
    // Array of shift amounts
    int shifts[] = {1, 2, 4, 8, 16, 32};
    uint64_t result = num;
    for (int shift : shifts) {
        result ^= (result << shift);
    }
    // 0000000000000000000010000000000000000000000000000000000000000000
    // after apply
    // 0000000000000000000011111111111111111111111111111111111111111111

    // 0001000000000000000000000000000000000000000000000000000000000000
    // after apply
    // 0001111111111111111111111111111111111111111111111111111111111111
    return result;
}
#+end_src
*** Sweeping
#+begin_src text
// 0000000000000000000010000000000000000000000000000000000000000000
// OR
// 0001000000000000000000000000000000000000000000000000000000000000
// result
// 0001000000000000000010000000000000000000000000000000000000000000

// 0000000000000000000011111111111111111111111111111111111111111111
// XOR
// 0001111111111111111111111111111111111111111111111111111111111111
// result
// 0001111111111111111100000000000000000000000000000000000000000000

#+end_src
*** Sweeping
#+begin_src text
Testing left shift:
Initial number:
0x00      00100000 00001000 00000100 00100000 00010000 10000000 00001010 10000100       0x07
After left shift by 1:
0x00      00110000 00001100 00000110 00110000 00011000 11000000 00001111 11000110       0x07
After left shift by 2:
0x00      00111100 00001111 00000111 10111100 00011110 11110000 00001100 00110111       0x07
After left shift by 4:
0x00      00111111 11001111 11110111 11000111 11011111 00011111 00001100 11110100       0x07
After left shift by 8:
0x00      00111111 11110000 00111000 00110000 00011000 11000000 00010011 11111000       0x07
After left shift by 16:
0x00      00111111 11110000 00000111 11000000 00100000 11110000 00001011 00111000       0x07
After left shift by 32:
0x00      00111111 11110000 00000111 11000000 00011111 00000000 00001100 11111000       0x07
#+end_src
*** Sweeping implemented by CLMUL, pclmulqdq
- Carry Less Multiply
- CLMUL(4, 20)
- 4 * 20
#+begin_src text
        4
X      20
----------
        4
X (16 + 4)
----------
       16
+      64
----------
       80
----------
#+end_src
*** Sweeping implemented by CLMUL, pclmulqdq
- CLMUL(4, 20)
- XOR ~= ADD
#+begin_src text
        00100  (4)
   X    10100  (20)
-------------
      00100__  (X4  means 4 << 2)
XOR 00100____  (X16 means 4 << 4)
-------------
    00101      (all XORed together)
-------------
#+end_src
*** Sweeping implemented by CLMUL, pclmulqdq
- CLMUL(inputJsonBitmask, EvenBitMask)
#+begin_src c++
int shifts[] = {1, 2, 4, 8, 16, 32};
uint64_t result = num;
for (int shift : shifts) {
    result ^= (result << shift);
}
// 01010101 01010101 01010101 01010101

//            input
// XOR (input <<  1)
// XOR (input <<  2)
// XOR (input <<  4)
// XOR (input <<  8)
// XOR (input << 16)
// XOR (input << 32)
// ----------------
//       quote mask
// ----------------
#+end_src
*** finally get quote mask
#+begin_src text
{ "\\\"Nam[{": [ 116,"\\\\" , 234, "true", false ], "t":"\\\"" }: input data
__1111111111_________11111_________11111____________11__11111___: CLMUL(Q,~0)
#+end_src
** Stage 1: 3 Character-Encoding Validation
1. Initial ASCII Fast Path, first bit == 0
2. Main algorithm
   1. Range check(0xF4 saturated subtract)
   2. Continuation Byte validation
*** Check for Ascii fast path
#+begin_src text
Single byte (ASCII):
0xxxxxxx                     (values 0-127)
Values start with 0, remaining 7 bits for data
#+end_src
*** Continuation Byte validation
#+begin_src text
Single byte (ASCII):
0xxxxxxx                     (values 0-127)
Values start with 0, remaining 7 bits for data

Two bytes:
110xxxxx 10xxxxxx           (values 128-2047)
First byte starts with 110

Three bytes:
1110xxxx 10xxxxxx 10xxxxxx  (values 2048-65535)
First byte starts with 1110

Four bytes:
1111xxxx 10xxxxxx 10xxxxxx 10xxxxxx   (values 65536+)
First byte starts with 11110
#+end_src
**** map to values
| high | Dec |   | high | Dec |
|------+-----+---+------+-----|
| 0000 |   1 |   | 1000 |   0 |
| 0001 |   1 |   | 1001 |   0 |
| 0010 |   1 |   | 1010 |   0 |
| 0011 |   1 |   | 1011 |   0 |
| 0100 |   1 |   | 1100 |   2 |
| 0101 |   1 |   | 1101 |   2 |
| 0110 |   1 |   | 1110 |   3 |
| 0111 |   1 |   | 1111 |   4 |

#+begin_src text
1111xxxx 10xxxxxx 10xxxxxx 10xxxxxx   (values 65536+)
4 0 0 0

1110xxxx 10xxxxxx 10xxxxxx  (values 2048-65535)
3 0 0
#+end_src
**** SIMD validation algorithm
#+begin_src text
4 0 0 0 3 0 0 2 0 1 1 1
  4 0 0 0 3 0 0 2 0 1 1 1 // <<= 1 byte, shift left by 1 byte
  3 0 0 0 2 0 0 1 0 0 0 0 // saturated subtract 1 from each byte

4 0 0 0 3 0 0 2 0 1 1 1
  3 0 0 0 2 0 0 1 0 0 0 0
4 3 0 0 3 2 0 2 1 1 1 1   // add it back into the original mapping

4 3 0 0 3 2 0 2 1 1 1 1   // add it back into the original mapping
    4 3 0 0 3 2 0 2 1 1 1 1   // <<= 2 byte, shift left by 2 bytes
    2 1 0 0 1 0 0 0 0 0 0 0   // saturated subtract 2
4 3 2 1 3 2 1 3 1 1 1 1   // add it back
// the end result will have no 0
// none of the numbers are bigger than the original
#+end_src
**** SIMD validation algorithm: Invalid example
#+begin_src text
2 0 0 0 4 3 0 0
  2 0 0 0 4 3 0 // shift left 1
  1 0 0 0 3 2 0 // saturated subtract 1
2 1 0 0 4 6 2 0

2 1 0 0 4 6 2 0
    0 0 2 1 0 0 4 6 // shift left 2
    0 0 0 0 0 0 2 4 // saturated subtract 2
2 1 0 0 4 6 4 4

2 0 0 0 4 3 0 0
2 1 0 0 4 6 4 4
    --- zeros found here invalid
          - 6 > 3
#+end_src
* Stage 2: Building the Tape
** Stage 2: 1 Number parsing
*** Understanding the is_all_digits
**** Overview
Fast 8 digit check
#+begin_src c++
uint64 high_nibble = val & 0xF0F0F0F0F0F0F0F0;
uint64 low_nibble = ((val + 0x0606060606060606) & 0xF0F0F0F0F0F0F0F0) >> 4;
uint64 combined = high_nibble | low_nibble;
bool is_all_digits = combined == 0x3333333333333333;
#+end_src
**** Key Insight: ASCII Characters from 0x29 to 0x3A
#+begin_notes
So there are 2 things to check, less than 0x30 which is 0x2F up here
0x3A which is bigger than 9 0x39 down here
#+end_notes
- notice all high nibble of valid digits are 3
| Char |  Hex | Binary    | Description   |                        |
|------+------+-----------+---------------+------------------------|
| '/'  | 0x2F | 0010 1111 | Forward Slash |                        |
|------+------+-----------+---------------+------------------------|
| '0'  | 0x30 | 0011 0000 | Digit Zero    | <-- Valid digits start |
| '1'  | 0x31 | 0011 0001 | Digit One     |                        |
| '2'  | 0x32 | 0011 0010 | Digit Two     |                        |
| '3'  | 0x33 | 0011 0011 | Digit Three   |                        |
| '4'  | 0x34 | 0011 0100 | Digit Four    |                        |
| '5'  | 0x35 | 0011 0101 | Digit Five    |                        |
| '6'  | 0x36 | 0011 0110 | Digit Six     |                        |
| '7'  | 0x37 | 0011 0111 | Digit Seven   |                        |
| '8'  | 0x38 | 0011 1000 | Digit Eight   |                        |
| '9'  | 0x39 | 0011 1001 | Digit Nine    | <-- Valid digits end   |
|------+------+-----------+---------------+------------------------|
| ':'  | 0x3A | 0011 1010 | Colon         |                        |

***** Step-by-Step Visual Explanation
****** Step 1: Initial masking of high nibbles
#+begin_src c++
uint64 high_nibble = val & 0xF0F0F0F0F0F0F0F0;
#+end_src
- if you are lesser than 0x3X, you are 0x2F,
- Let's take valid input "12345678":
#+BEGIN_EXAMPLE
Input bytes:    31 32 33 34 35 36 37 38
                || || || || || || || ||
                v| v| v| v| v| v| v| v|
High nibble:    3  3  3  3  3  3  3  3
                |  |  |  |  |  |  |  |
Mask:           F0 F0 F0 F0 F0 F0 F0 F0
                =  =  =  =  =  =  =  =
Result1:        30 30 30 30 30 30 30 30
#+END_EXAMPLE
****** How the low nibble check works
- we want to ensure that low nibble is within 0xX0 - 0xX9
  - 0xXA - 0xXF is illegal
    * Analyzing Carry Detection with Binary
****** Case 1: Valid Digit (0x39 = '9')
#+BEGIN_EXAMPLE
0x39 = 0011 1001  (Original value '9')
0x06 = 0000 0110  (Value we add)
      -----------
      0011 1111  (Result = 0x3F)
Low nibble does not overflow into high nibble and affect the 0x3 in high nibble

After masking high nibble (& 0xF0):
0x3F = 0011 1111
0xF0 = 1111 0000
      -----------
      0011 0000  (= 0x30)

After right shift by 4:
0x30 >> 4 = 0000 0011  (= 0x03) ✓ Valid!
#+END_EXAMPLE
****** Case 2: Invalid Character (0x3A = ':')
#+BEGIN_EXAMPLE
0x3A = 0011 1010  (Original value ':')
0x06 = 0000 0110  (Value we add)
      -----------
       0011 0000
          1 0000
      -----------
      0100 0000  (Result = 0x40) <- Notice the carry!
                                   The '1' carried into the high nibble

After masking high nibble (& 0xF0):
0x40 = 0100 0000
0xF0 = 1111 0000
      -----------
      0100 0000  (= 0x40)

After right shift by 4:
0x40 >> 4 = 0000 0100  (= 0x04) ✗ Invalid!

 0x3X
|0xX4
-----
 0x34 <- INVALID
-----
#+END_EXAMPLE

****** Step 2: Add 0x06 to detect non-digits
#+BEGIN_EXAMPLE
Low nibbles:    1  2  3  4  5  6  7  8
Add 0x06:       7  8  9  A  B  C  D  E
                ^  ^  ^  ^  ^  ^  ^  ^
                |  |  |  |  |  |  |  |
If original <= 9: No carry to high nibble
If original > 9: Carry affects high nibble
#+END_EXAMPLE

****** Step 3: Example with valid digits (0-9)
Take "12345678":
#+BEGIN_EXAMPLE

Original:       31 32 33 34 35 36 37 38
                v  v  v  v  v  v  v  v
high nibble:    30 30 30 30 30 30 30 30

Original:       31 32 33 34 35 36 37 38
After +0x06:    37 38 39 3A 3B 3C 3D 3E
Mask high:      30 30 30 30 30 30 30 30
low nibble:     03 03 03 03 03 03 03 03

high nibble:    30 30 30 30 30 30 30 30
low nibble:     03 03 03 03 03 03 03 03
OR together:    33 33 33 33 33 33 33 33
#+END_EXAMPLE

****** Step 4: Example with invalid character (';' = 0x3B)
Take "1234;678":
#+BEGIN_EXAMPLE
Original:       31 32 33 34 3B 36 37 38
After +0x06:    37 38 39 3A 41 3C 3D 3E
                               ^
                               |
Mask high:      30 30 30 30 40 30 30 30
                               ^ Different!
Shift right 4:  03 03 03 03 04 03 03 03
high nibble:    30 30 30 30 30 30 30 30
OR together:    33 33 33 33 34 33 33 33 ≠ 0x3333...
                               ^ Caught!
#+END_EXAMPLE

***** Why It Works
1. First part (val & 0xF0F0...):
   - Isolates high nibbles
   - Must be 0x30 for valid digits

2. Second part ((val + 0x06...) & 0xF0...):
   - Adding 0x06 to low nibble:
     - For 0-9: Result stays within nibble
     - For >9: Causes carry
   - After shift right 4:
     - Valid digits: Always 0x03
     - Invalid: Different value

3. When OR'd together:
   - Valid digits: Always 0x33
   - Invalid: Different pattern

***** Examples with Different Cases
****** Valid Cases
#+BEGIN_EXAMPLE
"00000000" -> 0x3333333333333333 ✓
"99999999" -> 0x3333333333333333 ✓
"12345678" -> 0x3333333333333333 ✓
#+END_EXAMPLE

****** Invalid Cases
#+BEGIN_EXAMPLE
"A" (0x41):
Original:  41
+0x06:     47
High:      40 ≠ 30 -> Fails

"/" (0x2F):
Original:  2F
+0x06:     35
High:      20 ≠ 30 -> Fails

":" (0x3A):
Original:  3A
+0x06:     40
High:      40 ≠ 30 -> Fails
#+END_EXAMPLE

***** Performance Benefits
- Single comparison instead of 8 individual checks
- No branches (important for modern CPUs)
- Uses native 64-bit operations
- Exploits CPU's ability to do parallel checks

This algorithm is a beautiful example of bit manipulation that turns what would normally be 8 comparisons into a single mathematical test.
*** Understanding SIMD-Based Fast Eight-Digit Number Parsing
**** Overview
Convert ASCII string of 8 digits to integer using SIMD instructions.
Example: "12345678" -> 12345678
#+begin_src c++
uint32_t parse_eight_digits_unrolled(char *chars) {
  __m128i ascii0 = _mm_set1_epi8(’0’);
  __m128i mul_1_10 = _mm_setr_epi8(10, 1, 10, 1, 10, 1, 10, 1, 10, 1, 10, 1, 10, 1, 10, 1);
  __m128i mul_1_100 = _mm_setr_epi16(100, 1, 100, 1, 100, 1, 100, 1);
  __m128i mul_1_10000 = _mm_setr_epi16(10000, 1, 10000, 1, 10000, 1, 10000, 1);
  __m128i number_ascii = _mm_loadu_si128((__m128i *)chars);
  __m128i in = _mm_sub_epi8(number_ascii, ascii0);
  __m128i t1 = _mm_maddubs_epi16(in, mul_1_10);
  __m128i t2 = _mm_madd_epi16(t1, mul_1_100);
  __m128i t3 = _mm_packus_epi32(t2, t2);
  __m128i t4 = _mm_madd_epi16(t3, mul_1_10000);
  return _mm_cvtsi128_si32(t4);
}
#+end_src

**** Step-by-Step Process
***** Step 1: Convert ASCII to Numeric Values
#+begin_src c++
  __m128i ascii0 = _mm_set1_epi8(’0’);
  __m128i number_ascii = _mm_loadu_si128((__m128i *)chars);
  __m128i in = _mm_sub_epi8(number_ascii, ascii0);
#+end_src

#+BEGIN_EXAMPLE
Input:          "12345678"
ASCII values:   31 32 33 34 35 36 37 38
Subtract:       30 30 30 30 30 30 30 30
Subtract '0':   01 02 03 04 05 06 07 08  (numeric values)
                |  |  |  |  |  |  |  |
Instruction:    _mm_sub_epi8 (PSUBB - packed subtract bytes)
#+END_EXAMPLE

***** Step 2: Multiply Alternate Digits by 10 and Add
#+begin_src c++
  __m128i mul_1_10 = _mm_setr_epi8(10, 1, 10, 1, 10, 1, 10, 1, 10, 1, 10, 1, 10, 1, 10, 1);
  __m128i t1 = _mm_maddubs_epi16(in, mul_1_10);
#+end_src
#+BEGIN_EXAMPLE
Values:         1  2  3  4  5  6  7  8
Multipliers:   10  1 10  1 10  1 10  1
                |  |  |  |  |  |  |  |
Results:       10  2 30  4 50  6 70  8
                \ /   \ /   \ /   \ /
Sums:           12    34    56    78     (as 16-bit values)

Instruction: _mm_maddubs_epi16 (PMADDUBSW - multiply and add unsigned bytes to signed words)
#+END_EXAMPLE

***** Step 3: Multiply Alternate 16-bit Values by 100
#+begin_notes
what is the next step ?
#+end_notes
#+begin_src c++
  __m128i mul_1_100 = _mm_setr_epi16(100, 1, 100, 1, 100, 1, 100, 1);
  __m128i t2 = _mm_madd_epi16(t1, mul_1_100);
#+end_src
#+BEGIN_EXAMPLE
Values:        12   34   56   78
Multipliers:  100    1  100    1
                |    |    |    |
Results:     1200   34 5600   78
                 \ /       \ /
Sums:           1234      5678    (as 32-bit values)

Instruction: _mm_madd_epi16 (PMADDWD - multiply and add packed words)
#+END_EXAMPLE
- what is the next step? 10000?
#+begin_src c++
  __m128i mul_1_10000 = _mm_setr_epi16(10000, 1, 10000, 1, 10000, 1, 10000, 1);
#+end_src

***** Step 4: Pack 32-bit Values to 16-bit
- reinterpret value as 32 bit instead of 16 bits!? why?
- so we can use ~_mm_setr_epi16~ instead of ~_mm_setr_epi32~
  - its more efficient
#+begin_src c++
    uint16 max_value = 65536;
  __m128i t3 = _mm_packus_epi32(t2, t2);
#+end_src
#+BEGIN_EXAMPLE
Before:   1234(32-bit)  5678(32-bit)
After:    1234(16-bit)  5678(16-bit)

Instruction: _mm_packus_epi32 (PACKUSDW - pack with unsigned saturation)
#+END_EXAMPLE

***** Step 5: Final Combine with Multiply by 10000
#+begin_src c++
  __m128i mul_1_10000 = _mm_setr_epi16(10000, 1, 10000, 1, 10000, 1, 10000, 1);
  __m128i t4 = _mm_madd_epi16(t3, mul_1_10000);
#+end_src
#+BEGIN_EXAMPLE
Values:        1234     5678
Multipliers:  10000        1
                  |        |
Results:   12340000     5678
                   \   /
Sum:           12345678    (final 32-bit result)

Instruction: _mm_madd_epi16 (PMADDWD again)
#+END_EXAMPLE
***** Summary: Why This is Fast
1. Parallel Processing:
   - Processes multiple digits simultaneously
   - Uses CPU's SIMD capabilities efficiently

2. Instruction Count:
   - Traditional: ~8 loads + ~8 multiplies + ~7 adds
   - SIMD: ~7 total instructions

3. Latency Analysis on Haswell:
   - PSUBB (subtract): 1 cycle
   - PMADDUBSW (multiply-add bytes): 5 cycles
   - PMADDWD (multiply-add words): 5 cycles
   - PACKUSDW (pack): 1 cycle
   - Total latency: ~17 cycles
* Actual implementation and optimization tricks in the code base
** inline
#+begin_src c++
#elif defined(__GNUC__) && !defined(__OPTIMIZE__)
  // If optimizations are disabled, forcing inlining can lead to significant
  // code bloat and high compile times. Don't use simdjson_really_inline for
  // unoptimized builds.
  #define simdjson_inline inline
#else
#+end_src
** static cast
#+begin_src c++
reinterpret_cast (Used in the SIMD code):

static simdjson_inline simd8<T> load(const T values[32]) {
    return _mm256_loadu_si256(reinterpret_cast<const __m256i *>(values));
}

Zero runtime cost - purely tells compiler to treat bits as different type
No actual data conversion
Most dangerous cast - must ensure types are compatible bit-wise
Common in low-level/SIMD code where you know exact memory layout
#+end_src

#+begin_src c++
template <typename Child> struct base {
  __m256i value;

  // This cast gets resolved at compile time with no runtime overhead
  simdjson_inline operator const __m256i &() const { return this->value; }

  // The Child type is known at compile time
  simdjson_inline Child operator|(const Child other) const {
    return _mm256_or_si256(*this, other);
  }
}
#+end_src

#+begin_src c++
// With dynamic (runtime) casting:
class Base { virtual ~Base() {} };
class Derived : public Base { };

void process(Base* ptr) {
    Derived* d = dynamic_cast<Derived*>(ptr);  // Runtime check
    if(d) { /* use d */ }
}

// With static (compile-time) casting like simdjson:
template <typename T>
struct base {
    T* as_child() {
        return static_cast<T*>(this);  // No runtime check needed
    }
};

struct derived : base<derived> {
};

void process(derived& d) {
    // Compiler knows exact type, no checks needed
    auto* ptr = d.as_child();  // Compiles to essentially nothing
}
#+end_src
Dynamic casting needs runtime type information (RTTI) and checks
Static casting is just the compiler calculating new memory addresses
Static casting can often be optimized away entirely
No runtime overhead for type safety with static casting

Compile to essentially the same assembly as if you had written the operations directly on the raw types - there's no casting overhead in the final machine code.
*** about virtual functions
early and late binding
#+begin_src text
Key differences and use cases:

Regular Functions:


Faster performance (direct function call)
Smaller object size (no vtable pointer)
Binding decided at compile time
Used when you want static/fixed behavior
Used when performance is critical (like in simdjson)


Virtual Functions:


Runtime polymorphism (behavior can change based on actual object type)
Allows for interface-based programming
Enables "plugin" or extensible architectures
Used when you need flexibility and dynamic behavior
Common in frameworks and large-scale OOP designs
#+end_src
** simd8
Let me explain the CRTP (Curiously Recurring Template Pattern) being used in this code:

```cpp
// Base template
template <typename Child> struct base {
    __m256i value;

    // The base class methods use Child type
    simdjson_inline Child operator|(const Child other) const {
        return _mm256_or_si256(*this, other);
    }

    // Uses static_cast to Child
    simdjson_inline Child &operator|=(const Child other) {
        auto this_cast = static_cast<Child *>(this);
        *this_cast = *this_cast | other;
        return *this_cast;
    }
};

// Child class that inherits from base with itself as template parameter
struct simd8<uint8_t> : base8_numeric<uint8_t> {
    // Implementation specific to uint8_t
};
```

Key aspects:

1. Static Polymorphism
- No virtual functions - everything resolved at compile time
- Child type is known in base class methods
- Compiler can inline and optimize everything

2. Type Safety
```cpp
simdjson_inline Child operator|(const Child other) const {
    return _mm256_or_si256(*this, other);
}
```
- Return type is correct derived class
- Arguments must match derived class type
- Caught at compile time if types don't match

3. Static Casting
```cpp
auto this_cast = static_cast<Child *>(this);
```
- Safe because Child is guaranteed to be derived class
- No runtime overhead - resolved at compile time
- Allows access to derived class methods/members

4. Inheritance Chain Example:
```cpp
base<simd8<uint8_t>>     // Base template
    ↑
base8<uint8_t>          // Adds common SIMD operations
    ↑
base8_numeric<uint8_t>  // Adds numeric operations
    ↑
simd8<uint8_t>         // Final implementation
```

5. Benefits in this Code:
- Zero overhead abstraction
- Type-safe SIMD operations
- Reuse of common SIMD code
- Specialized implementations where needed
- Compile-time polymorphism

This pattern is particularly useful for SIMD code because:
1. You want zero runtime overhead
2. Operations are known at compile time
3. Type safety is important
4. Code reuse across different SIMD types
5. Specialized implementations for different types
